{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZAAQH_AnX1"
      },
      "source": [
        "# Algoritmo de BackPropagation multiclase\n",
        "\n",
        "Este codigo se utilizará para entrenar la red neuronal de clasificacion de Nuestras Caras\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3MCES9WMJEw"
      },
      "source": [
        "### Código en Python del Algoritmo de BackPropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsDfkqXaZR5q"
      },
      "source": [
        "La clase del dataset debe ser categorica\n",
        "Existe una clase en Python que resuelve el problema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0XkHjieK2xD"
      },
      "outputs": [],
      "source": [
        "# conexion al Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/.drive')\n",
        "!mkdir -p \"/content/.drive/My Drive/DMA\"\n",
        "!mkdir -p \"/content/buckets\"\n",
        "!ln -s \"/content/.drive/My Drive/DMA\" /content/buckets/b1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtiWbym5uwI6"
      },
      "outputs": [],
      "source": [
        "# instalo  itables solo si no esta instalado\n",
        "!pip show itables >/dev/null || pip install itables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejqYzmGGLADY"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "from functools import reduce\n",
        "from itables import init_notebook_mode\n",
        "init_notebook_mode(all_interactive=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bYjXyHqLQFe"
      },
      "outputs": [],
      "source": [
        "# definicion de la clase de graficos\n",
        "\n",
        "class perceptron_plot:\n",
        "    \"\"\"plotting first hidden layer class\"\"\"\n",
        "    def __init__(self, X, Y, delay) -> None:\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.delay = delay\n",
        "        x1_min = np.min(X[:,0])\n",
        "        x2_min = np.min(X[:,1])\n",
        "        x1_max = np.max(X[:,0])\n",
        "        x2_max = np.max(X[:,1])\n",
        "        self.x1_min = x1_min - 0.1*(x1_max - x1_min)\n",
        "        self.x1_max = x1_max + 0.1*(x1_max - x1_min)\n",
        "        self.x2_min = x2_min - 0.1*(x2_max - x2_min)\n",
        "        self.x2_max = x2_max + 0.1*(x2_max - x2_min)\n",
        "        self.fig = plt.figure(figsize = (10,8))\n",
        "        self.ax = self.fig.subplots()\n",
        "        self.ax.set_xlim(self.x1_min, self.x1_max, auto=False)\n",
        "        self.ax.set_ylim(self.x2_min, self.x2_max, auto=False)\n",
        "\n",
        "    def graficarVarias(self, W, x0, epoch, error) -> None:\n",
        "        display.clear_output(wait =True)\n",
        "        plt.cla()\n",
        "        #self.ax = self.fig.subplots()\n",
        "\n",
        "        self.ax.set_xlim(self.x1_min, self.x1_max)\n",
        "        self.ax.set_ylim(self.x2_min, self.x2_max)\n",
        "        plt.title( 'epoch ' + str(epoch) + '  reg ' + \"{0:.2E}\".format(error))\n",
        "        # ploteo puntos\n",
        "        num_classes = len(np.unique(self.Y))\n",
        "        # mycolors = plt.cm.get_cmap('tab10', num_classes)\n",
        "        unique_labels = np.unique(self.Y)\n",
        "        label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
        "        numerical_labels = np.array([label_mapping[label] for label in self.Y])\n",
        "        # scatter = self.ax.scatter(self.X[:,0], self.X[:,1], c=self.Y, s=20)\n",
        "        scatter = self.ax.scatter(self.X[:,0], self.X[:,1], c=numerical_labels, s=20, cmap=plt.get_cmap('viridis', num_classes))\n",
        "        # self.ax.plot(self.X[:,0], self.X[:,1], 'o', c=vcolores,  markersize=2)\n",
        "\n",
        "\n",
        "        # dibujo las rectas\n",
        "        for i in range(len(x0)):\n",
        "            #vx2_min = -(W[0,i]*self.x1_min + x0[i])/W[1,i]\n",
        "            #vx2_max = -(W[0,i]*self.x1_max + x0[i])/W[1,i]\n",
        "            vx2_min = -(W[i,0]*self.x1_min + x0[i])/W[i,1]\n",
        "            vx2_max = -(W[i,0]*self.x1_max + x0[i])/W[i,1]\n",
        "\n",
        "            self.ax.plot([self.x1_min, self.x1_max],\n",
        "                         [vx2_min, vx2_max],\n",
        "                         linewidth = 2,\n",
        "                         color = 'red',\n",
        "                         alpha = 0.5)\n",
        "\n",
        "        display.display(plt.gcf())\n",
        "        #plt.cla()\n",
        "        time.sleep(self.delay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DgbLgqe3gTp"
      },
      "outputs": [],
      "source": [
        "# definicion de las funciones de activacion\n",
        "#  y sus derivadas\n",
        "#  ahora agregando las versiones VECTORIZADAS\n",
        "\n",
        "def func_eval(fname, x):\n",
        "    match fname:\n",
        "        case \"purelin\":\n",
        "            y = x\n",
        "        case \"logsig\":\n",
        "            y = 1.0 / ( 1.0 + math.exp(-x) )\n",
        "        case \"tansig\":\n",
        "            y = 2.0 / ( 1.0 + math.exp(-2.0*x) ) - 1.0\n",
        "    return y\n",
        "\n",
        "# version vectorizada de func_eval\n",
        "func_eval_vec = np.vectorize(func_eval)\n",
        "\n",
        "\n",
        "def deriv_eval(fname, y):  #atencion que y es la entrada y=f( x )\n",
        "    match fname:\n",
        "        case \"purelin\":\n",
        "            d = 1.0\n",
        "        case \"logsig\":\n",
        "            d = y*(1.0-y)\n",
        "        case \"tansig\":\n",
        "            d = 1.0 - y*y\n",
        "    return d\n",
        "\n",
        "\n",
        "# version vectorizada de deriv_eval\n",
        "deriv_eval_vec = np.vectorize(deriv_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXinnKQIFnxU"
      },
      "source": [
        "### Clase  multiperceptron\n",
        "entrenar, predecir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gv7esD65dqu"
      },
      "outputs": [],
      "source": [
        "# definicion de la clase de multiperceptron\n",
        "\n",
        "class multiperceptron(object):\n",
        "    \"\"\"Multiperceptron class\"\"\"\n",
        "\n",
        "    # inicializacion de los pesos de todas las capas\n",
        "    def _red_init(self, semilla) -> None:\n",
        "        niveles = self.red['arq']['layers_qty']\n",
        "\n",
        "        np.random.seed(semilla)\n",
        "        for i in range(niveles):\n",
        "           nivel = dict()\n",
        "           nivel['id'] = i\n",
        "           nivel['last'] = (i==(niveles-1))\n",
        "           nivel['size'] = self.red[\"arq\"][\"layers_size\"][i]\n",
        "           nivel['func'] = self.red[\"arq\"][\"layers_func\"][i]\n",
        "\n",
        "           if( i==0 ):\n",
        "              entrada_size = self.red['arq']['input_size']\n",
        "           else:\n",
        "              entrada_size =  self.red['arq']['layers_size'][i-1]\n",
        "\n",
        "           salida_size =  nivel['size']\n",
        "\n",
        "           # los pesos, inicializados random\n",
        "           nivel['W'] = np.random.uniform(-0.5, 0.5, [salida_size, entrada_size])\n",
        "           nivel['w0'] = np.random.uniform(-0.5, 0.5, [salida_size, 1])\n",
        "\n",
        "           # los momentos, inicializados en CERO\n",
        "           nivel['W_m'] = np.zeros([salida_size, entrada_size])\n",
        "           nivel['w0_m'] = np.zeros([salida_size, 1])\n",
        "\n",
        "           self.red['layer'].append(nivel)\n",
        "\n",
        "    # constructor generico\n",
        "    def __init__(self) -> None:\n",
        "        self.data = dict()\n",
        "        self.red = dict()\n",
        "        self.carpeta = \"\"\n",
        "\n",
        "\n",
        "    # inicializacion full\n",
        "    def inicializar(self, df, campos, clase, hidden_layers_sizes, layers_func,\n",
        "                 semilla, carpeta) -> None:\n",
        "\n",
        "        # genero self.data\n",
        "        self.data['X'] = np.array( df.select(campos))\n",
        "        X_mean = self.data['X'].mean(axis=0)\n",
        "        X_sd = self.data['X'].std(axis=0)\n",
        "        self.data['X'] = (self.data['X'] - X_mean)/X_sd\n",
        "\n",
        "        #  Ylabel en  numpy\n",
        "        label =df.select(clase)\n",
        "        self.data['Ylabel'] = np.array(label).reshape(len(label))\n",
        "\n",
        "        # one-hot-encoding de Y\n",
        "        col_originales = df.columns\n",
        "        self.data['Y'] = np.array( df.to_dummies(clase).drop(col_originales, strict=False) )\n",
        "\n",
        "        col_dummies = sorted( list( set(df.to_dummies(clase).columns) -  set(col_originales)))\n",
        "        clases_originales = reduce(lambda acc, x: acc + [x[(len(clase)+1):]], col_dummies, [])\n",
        "\n",
        "        tamanos = hidden_layers_sizes\n",
        "        tamanos.append(self.data['Y'].shape[1])\n",
        "\n",
        "        arquitectura = {\n",
        "             'input_size' : self.data['X'].shape[1],\n",
        "             'input_mean' : X_mean,\n",
        "             'input_sd' :  X_sd,\n",
        "             'output_values' : clases_originales,\n",
        "             'layers_qty' : len(hidden_layers_sizes), # incluye la capa de salida, pero no la de entrada\n",
        "             'layers_size' : tamanos ,\n",
        "             'layers_func' : layers_func,\n",
        "        }\n",
        "\n",
        "        self.red['arq'] = arquitectura\n",
        "\n",
        "\n",
        "        # inicializo  work\n",
        "        self.red['work'] = dict()\n",
        "        self.red['work']['epoch'] = 0\n",
        "        self.red['work']['MSE'] = float('inf')\n",
        "        self.red['work']['train_error_rate'] = float('inf')\n",
        "\n",
        "        self.red['layer'] = list()\n",
        "        self._red_init(semilla)\n",
        "\n",
        "        # grabo el entorno\n",
        "        self.carpeta = carpeta\n",
        "        os.makedirs(self.carpeta, exist_ok=True)\n",
        "        with open(self.carpeta+\"/data.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.data, f)\n",
        "\n",
        "        with open(self.carpeta+\"/red.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.red, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Algoritmo Backpropagation\n",
        "    def  entrenar(self, epoch_limit, MSE_umbral,\n",
        "               learning_rate, lr_momento, save_frequency,\n",
        "               retomar=True) -> None:\n",
        "\n",
        "        # si debo retomar\n",
        "        if( retomar):\n",
        "            with open(self.carpeta+\"/data.pkl\", 'rb') as f:\n",
        "              self.data = pickle.load(f)\n",
        "\n",
        "            with open(self.carpeta+\"/red.pkl\", 'rb') as f:\n",
        "              self.red = pickle.load(f)\n",
        "\n",
        "\n",
        "        # inicializaciones del bucle principal del backpropagation\n",
        "        epoch = self.red['work']['epoch']\n",
        "        MSE = self.red['work']['MSE']\n",
        "\n",
        "        # inicializacion del grafico\n",
        "        grafico = perceptron_plot(X=self.data['X'], Y=self.data['Ylabel'], delay=0.1)\n",
        "\n",
        "        # continuo mientras error cuadratico medio muy grande  y NO llegué al límite de epochs\n",
        "        Xfilas = self.data['X'].shape[0]\n",
        "        niveles = self.red[\"arq\"][\"layers_qty\"]\n",
        "\n",
        "        while ( MSE > MSE_umbral) and (epoch < epoch_limit) :\n",
        "          epoch += 1\n",
        "\n",
        "\n",
        "          # recorro siempre TODOS los registros de entrada\n",
        "          for fila in range(Xfilas):\n",
        "             # fila es el registro actual\n",
        "             x = self.data['X'][fila:fila+1,:]\n",
        "             clase = self.data['Y'][fila:fila+1,:]\n",
        "\n",
        "             # propagar el x hacia adelante, FORWARD\n",
        "             entrada = x.T  # la entrada a la red\n",
        "\n",
        "             # etapa forward\n",
        "             # recorro hacia adelante, nivel a nivel\n",
        "             vsalida =  [0] *(niveles) # salida de cada nivel de la red\n",
        "\n",
        "             for i in range(niveles):\n",
        "               estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "               vsalida[i] =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "               entrada = vsalida[i]  # para la proxima vuelta\n",
        "\n",
        "\n",
        "             # etapa backward\n",
        "             # calculo los errores en la capa hidden y la capa output\n",
        "             verror =  [0] *(niveles+1) # inicializo dummy\n",
        "             verror[niveles] = clase.T - vsalida[niveles-1]\n",
        "\n",
        "             i = niveles-1\n",
        "             verror[i] = verror[i+1] * deriv_eval_vec(self.red['layer'][i]['func'], vsalida[i])\n",
        "\n",
        "             for i in reversed(range(niveles-1)):\n",
        "               verror[i] = deriv_eval_vec(self.red['layer'][i]['func'], vsalida[i])*(self.red['layer'][i+1]['W'].T @ verror[i+1])\n",
        "\n",
        "             # ya tengo los errores que comete cada capa\n",
        "             # corregir matrices de pesos, voy hacia atras\n",
        "             # backpropagation\n",
        "             entrada = x.T\n",
        "             for i in range(niveles):\n",
        "               self.red['layer'][i]['W_m'] = learning_rate *(verror[i] @ entrada.T) + lr_momento *self.red['layer'][i]['W_m']\n",
        "               self.red['layer'][i]['w0_m'] = learning_rate * verror[i] + lr_momento * self.red['layer'][i]['w0_m']\n",
        "\n",
        "               self.red['layer'][i]['W']  =  self.red['layer'][i]['W'] + self.red['layer'][i]['W_m']\n",
        "               self.red['layer'][i]['w0'] =  self.red['layer'][i]['w0'] + self.red['layer'][i]['w0_m']\n",
        "               entrada = vsalida[i]  # para la proxima vuelta\n",
        "\n",
        "\n",
        "\n",
        "          # ya recalcule las matrices de pesos\n",
        "          # ahora avanzo la red, feed-forward\n",
        "          # para calcular el red(X) = Y\n",
        "          entrada = self.data['X'].T\n",
        "          for i in range(niveles):\n",
        "            estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "            salida =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "            entrada = salida  # para la proxima vuelta\n",
        "\n",
        "          # calculo el error cuadratico medio TODOS los X del dataset\n",
        "          MSE= np.mean( (self.data['Y'].T - salida)**2 )\n",
        "\n",
        "          # Grafico las rectas SOLAMENTE de la Primera Hidden Layer\n",
        "          # tengo que hacer w0.T[0]  para que pase el vector limpio\n",
        "          if( epoch % save_frequency == 0 ) or ( MSE <= MSE_umbral) or (epoch >= epoch_limit) :\n",
        "              # grafico\n",
        "              W = self.red['layer'][0]['W']\n",
        "              w0 = self.red['layer'][0]['w0']\n",
        "              grafico.graficarVarias(W, w0.T[0], epoch, MSE)\n",
        "\n",
        "              # almaceno en work\n",
        "              self.red['work']['epoch'] = epoch\n",
        "              self.red['work']['MSE'] = MSE\n",
        "              prediccion = np.argmax( salida.T, axis=1)\n",
        "              # prediccion\n",
        "              out = np.array(self.red[\"arq\"]['output_values'])\n",
        "              error_rate = np.mean( self.data['Ylabel'] != out[prediccion])\n",
        "              self.red[\"work\"]['train_error_rate'] = error_rate # error_rate != error cuadratico medio\n",
        "\n",
        "              # grabo a un archivo la red neuronal  entrenada por donde esté\n",
        "              #   solo la red, NO los datos\n",
        "              with open(carpeta+\"/red.pkl\", 'wb') as f:\n",
        "                 pickle.dump(self.red, f)\n",
        "\n",
        "        return (epoch, MSE, self.red['work']['train_error_rate'] )\n",
        "\n",
        "\n",
        "    # predigo a partir de modelo recien entrenado\n",
        "    def  predecir(self, df_new, campos, clase) -> None:\n",
        "        niveles = self.red['arq']['layers_qty']\n",
        "\n",
        "        # etapa forward\n",
        "        # recorro hacia adelante, nivel a nivel\n",
        "        X_new =  np.array( df_new.select(campos))\n",
        "\n",
        "\n",
        "        # estandarizo manualmente\n",
        "        #  con las medias y desvios que almacene durante el entrenamiento\n",
        "        X_new = (X_new - self.red['arq']['input_mean'])/self.red['arq']['input_sd']\n",
        "\n",
        "        # grafico los datos nuevos\n",
        "        Ylabel_new =df_new.select(clase)\n",
        "        Ylabel_new = np.array(Ylabel_new).reshape(len(Ylabel_new))\n",
        "        grafico = perceptron_plot(X=X_new, Y=Ylabel_new, delay=0.1)\n",
        "        W = self.red['layer'][0]['W']\n",
        "        w0 = self.red['layer'][0]['w0']\n",
        "        grafico.graficarVarias(W, w0.T[0], epoch, MSE)\n",
        "\n",
        "        # la entrada a la red,  el X que es TODO  x_new\n",
        "        entrada = X_new.T  # traspongo, necesito vectores columna\n",
        "\n",
        "        for i in range(niveles):\n",
        "          estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "          salida =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "          entrada = salida  # para la proxima vuelta\n",
        "\n",
        "        # me quedo con la neurona de la ultima capa que se activio con mayor intensidad\n",
        "        pred_idx = np.argmax( salida.T, axis=1)\n",
        "        pred_raw = np.max( salida.T, axis=1)\n",
        "\n",
        "        # calculo error_rate\n",
        "        out = np.array(self.red['arq']['output_values'])\n",
        "        error_rate = np.mean( np.array(df_new.select(\"y\") != out[pred_idx]))\n",
        "\n",
        "        return (out[pred_idx], pred_raw, error_rate)\n",
        "\n",
        "\n",
        "    # cargo un modelo ya entrenado, grabado en carpeta\n",
        "    def cargar_modelo(self, carpeta) -> None:\n",
        "        self.carpeta = carpeta\n",
        "\n",
        "        with open(self.carpeta+\"/red.pkl\", 'rb') as f:\n",
        "          self.red = pickle.load(f)\n",
        "\n",
        "        return (self.red['work']['epoch'],\n",
        "                self.red['work']['MSE'],\n",
        "                self.red['work']['train_error_rate'] )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u3z27yD4aoF"
      },
      "source": [
        "## 1 Lectura del Dataset\n",
        "\n",
        "En este humilde y restringida version, la clase del dataset debe ser categorica, no es capaz de trabajar con clases continuas.\n",
        "La clase categorica puede ser  n-aria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZAoHm3HSwAQ"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/josekeh/dma_g2/main/input/caras.pkl'\n",
        "!wget 'https://raw.githubusercontent.com/josekeh/dma_g2/main/input/caras50.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95kI-lzePHom"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"caras50-megalimpio.pkl\", \"rb\") as file:\n",
        "    caras = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SpZ4JF8pLD11"
      },
      "outputs": [],
      "source": [
        "# Lectura del dataset con la moderna libreria Polars  (Pandas debe extinguirse!)\n",
        "\n",
        "# df = pl.read_csv('https://raw.githubusercontent.com/josekeh/dma_g2/main/input/isomap_df.csv')\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HM107y_4gzg"
      },
      "source": [
        "### 1.1 Particion  training/testing\n",
        "\n",
        "\n",
        "\n",
        "*   Es valido cambiar la *semilla_particion* para probar distintos <test, train> y asi estimar con mas precisión el error rate en testing  (Montecarlo Estimation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juIGBXiv64Lv"
      },
      "outputs": [],
      "source": [
        "# CODIGO PROFE\n",
        "# # particion del dataset en training/testing\n",
        "\n",
        "# # semilla_particion = 102191\n",
        "# semilla_particion = 19961002\n",
        "# pct_train = 0.75  # ratio de registros que va a training\n",
        "\n",
        "\n",
        "# def train_test_split_df(df, seed=0, test_size=0.2):\n",
        "#     return df.with_columns(\n",
        "#         pl.int_range(pl.len(), dtype=pl.UInt32)\n",
        "#         .shuffle(seed=seed)\n",
        "#         .gt(pl.len() * test_size)\n",
        "#         .alias(\"split\")\n",
        "#     ).partition_by(\"split\", include_key=False)\n",
        "\n",
        "\n",
        "# (df_train, df_test) = train_test_split_df(df,\n",
        "#                                           seed=semilla_particion,\n",
        "#                                           test_size=pct_train)\n",
        "\n",
        "# # imprimo los tamaños\n",
        "# print(\"Train:\", df_train.shape)\n",
        "# print(\"Test:\", df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpEKQaxEX25a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Algoritmo para chequear la distribución de nombres\n",
        "def check_separar(training_nombres):\n",
        "  # Contar la frecuencia de cada palabra\n",
        "  contador = Counter(training_nombres)\n",
        "  total_palabras = sum(contador.values())\n",
        "  cantidad_diferentes = len(contador)  # Número de palabras únicas\n",
        "\n",
        "  # Convertir a porcentaje\n",
        "  frecuencias = {palabra: (conteo / total_palabras) * 100 for palabra, conteo in contador.items()}\n",
        "\n",
        "  umbral = 100 / cantidad_diferentes\n",
        "\n",
        "  # Mostrar el gráfico si no es muy dispar\n",
        "  if (min(frecuencias.values()) < umbral*.75 or max(frecuencias.values()) > umbral*1.25):\n",
        "    return 0\n",
        "  else:\n",
        "    # Graficar\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(frecuencias.keys(), frecuencias.values(), color='skyblue')\n",
        "\n",
        "    # Agregar línea horizontal en 100% / cantidad de palabras diferentes\n",
        "    plt.axhline(y=umbral, color='red', linestyle='--', label=f\"Ideal: {umbral:.2f}%\")\n",
        "\n",
        "    # Etiquetas\n",
        "    plt.xlabel(\"Persona\")\n",
        "    plt.ylabel(\"Frecuencia (%)\")\n",
        "    plt.title(\"Distribución del training set según las personas\")\n",
        "    plt.ylim(0, max(frecuencias.values()) + 5)  # Ajuste del eje Y\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "# Algoritmo para separar training y testing suponiendo 2 listas\n",
        "def separar_aleatorio(lista_caras, lista_nombres, porcentaje_training):\n",
        "    if (len(lista_caras) != len(lista_nombres)):\n",
        "      print(\"Error las listas deben tener la misma longitud\")\n",
        "    else:\n",
        "      chequeo = 0\n",
        "\n",
        "      while (chequeo == 0):\n",
        "        training_caras = []\n",
        "        training_nombres = []\n",
        "        testing_caras = []\n",
        "        testing_nombres = []\n",
        "\n",
        "        for i in range(len(lista_caras)):\n",
        "          aleatorio = random.random()\n",
        "          if aleatorio <= porcentaje_training:\n",
        "            training_caras.append(lista_caras[i])\n",
        "            training_nombres.append(lista_nombres[i])\n",
        "          else:\n",
        "            testing_caras.append(lista_caras[i])\n",
        "            testing_nombres.append(lista_nombres[i])\n",
        "        chequeo = check_separar(training_nombres)\n",
        "\n",
        "      return training_caras, training_nombres, testing_caras, testing_nombres\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy5Pg8D1TnYP"
      },
      "source": [
        "### ISOMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFQHAaAJTmue"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import Isomap\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaccTM9Sn9xN"
      },
      "outputs": [],
      "source": [
        "def procesar_isomap(train_caras, train_nombres, test_caras, test_nombres, mejor_n_components, mejor_n_neighbors):\n",
        "  data_training = []\n",
        "  for j in range(len(train_caras)):\n",
        "    data_training.append(train_caras[j])\n",
        "\n",
        "  data_testing = []\n",
        "  for j in range(len(test_caras)):\n",
        "    data_testing.append(test_caras[j])\n",
        "\n",
        "  # Convertimos las listas en matrices de tamaño (n, 900)\n",
        "  data_training = np.vstack(data_training)\n",
        "  data_testing = np.vstack(data_testing)\n",
        "\n",
        "  isomap = Isomap(n_components=mejor_n_components, n_neighbors=mejor_n_neighbors)\n",
        "  output_isomap_training = isomap.fit_transform(data_training)\n",
        "\n",
        "  output_isomap_testing = isomap.transform(data_testing)\n",
        "\n",
        "  df_train = pl.DataFrame(output_isomap_training)\n",
        "  df_train.columns = ['x' + str(i + 1) for i in range(df_train.shape[1])]\n",
        "  df_train = df_train.with_columns(pl.Series(train_nombres).alias(\"y\"))\n",
        "\n",
        "  df_test = pl.DataFrame(output_isomap_testing)\n",
        "  df_test.columns = ['x' + str(i + 1) for i in range(df_test.shape[1])]\n",
        "  df_test = df_test.with_columns(pl.Series(test_nombres).alias(\"y\"))\n",
        "\n",
        "  return df_train, df_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j_YyhttTxA1"
      },
      "source": [
        "### Construcción de"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVDBHdeF_Ai"
      },
      "source": [
        "## 2  Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH__fEUO4mED"
      },
      "source": [
        "### 2.1  Inicializacion de la neural network\n",
        "\n",
        "\n",
        "\n",
        "*   Es valido cambiar la *semilla_red* para arrancar el entrenamiento con distintas rectas iniciales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9LvxyY_mp-n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FBNjEaQCXIH"
      },
      "outputs": [],
      "source": [
        "# defino la red multiperceptron\n",
        "carpeta = \"/content/buckets/b1/nn01/\"  # cambiar con cada corrida\n",
        "semilla_red = 200177  # define las rectas iniciales\n",
        "\n",
        "# una sola capa oculta de 2 neuronas  [2]\n",
        "# la capa oculta y la final tienen ambas logsig de activacion\n",
        "mp = multiperceptron()\n",
        "mp.inicializar(\n",
        "    df=df_train, campos=campos, clase=\"y\",  # especificaion del dataset\n",
        "    hidden_layers_sizes=[20 ,13],  # no va la capa final, solo hidden layers\n",
        "    layers_func=['logsig','logsig','logsig'], # funciones de activacion de cada capa\n",
        "    semilla=semilla_red,\n",
        "    carpeta = carpeta\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA1Nm1yWm-Le"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ8J9L7onBUI"
      },
      "source": [
        "### 2.2 Entrenamiento de la neural network = backpropagation (con gridsearch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulx9POPS5i0W"
      },
      "outputs": [],
      "source": [
        "def campos_func(n):\n",
        "  campos = []\n",
        "  for i in range(n):\n",
        "    campos.append(f\"x{i+1}\")\n",
        "  return campos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqPhyfU9I01B"
      },
      "outputs": [],
      "source": [
        "# Corridas\n",
        "file_path = '/content/buckets/b1/DMA/NuestrasCaras/corridas.csv'\n",
        "try:\n",
        "  df = pl.read_csv(file_path)\n",
        "  list_of_iters = df.drop(\"epoch\", \"error_rate\").rows()\n",
        "except Exception as e:\n",
        "  print(f\"Error reading the CSV file: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-7yACHCnFvA"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "inicio = datetime.datetime.now()\n",
        "\n",
        "# seeds = [200177] # seeds a utilizar\n",
        "# learning_rates = [0.2] #learning rates a probar\n",
        "# epochs = [200,400] #Límites de epochs a usar\n",
        "# umbrales = [0.00001] # umbrales de mse a probar\n",
        "# capas = [1] # capas a usar, sin contar la última oculta, o sea, la red va a tener capa[i] + 1 (verificamos que usar una más overfitea mucho)\n",
        "# neuronas = [20] # Neuronas a usar en las capas declaradas anteriormente, por ahora dejamos fija la última oculta con 13 (a revisar)\n",
        "# isomap_comp = [35] # n_components de isomap a probar, por ahora mantenemos el número de vecinos fijos\n",
        "# contador = 1 #Contador solamente para guardar los diferentes pkl de las redes\n",
        "# check_iter_list = 'list_of_iters' in locals()\n",
        "# func_out = ['logsig', 'tansig']\n",
        "seeds = [200177] # seeds a utilizar\n",
        "learning_rates = [0.2] #learning rates a probar\n",
        "epochs = [100, 500, 700, 1000, 1500] #Límites de epochs a usar, PONER EN ORDEN CRECIENTE\n",
        "umbrales = [0.0001] # umbrales de mse a probar\n",
        "capas = [1] # capas a usar, sin contar la última oculta, o sea, la red va a tener capa[i] + 1 (verificamos que usar una más overfitea mucho)\n",
        "neuronas = [20] # Neuronas a usar en las capas declaradas anteriormente, por ahora dejamos fija la última oculta con 13 (a revisar)\n",
        "isomap_comp = [80] # n_components de isomap a probar, por ahora mantenemos el número de vecinos fijos\n",
        "contador = 0 #Contador solamente para guardar los diferentes pkl de las redes\n",
        "check_iter_list = 'list_of_iters' in locals()\n",
        "func_out = ['logsig']\n",
        "\n",
        "# Creamos las listas de nombres y caras para usar en la red\n",
        "faces = [sublista[0] for sublista in caras]\n",
        "names = [sublista[1] for sublista in caras]\n",
        "\n",
        "# Creación del df para guardar los resultados que vamos obteniendo (y no morir en el intento)\n",
        "# seeds <- epochs <- lr <- umbral <- neuronas <- capas (ToDo: revisar para reciclar epochs y particiones de seed, ahora está haciendo uno por cada iter)\n",
        "\n",
        "data = {'seed': [], 'capa': [], 'neurona': [], 'umbral': [], 'lr': [], 'epoch': [], 'epoch_lim': [],  'componente': [], 'error_rate': [], 'func': []}\n",
        "df_results = pl.DataFrame(data)\n",
        "\n",
        "df_results = df_results.with_columns([\n",
        "    pl.col(\"epoch\").cast(pl.Int64),\n",
        "    pl.col(\"epoch_lim\").cast(pl.Int64),\n",
        "    pl.col(\"seed\").cast(pl.Int64),\n",
        "    pl.col(\"capa\").cast(pl.Int64),\n",
        "    pl.col(\"neurona\").cast(pl.Int64),\n",
        "    pl.col(\"componente\").cast(pl.Int64),\n",
        "    pl.col(\"error_rate\").cast(pl.Float64),\n",
        "    pl.col(\"umbral\").cast(pl.Float64),\n",
        "    pl.col(\"lr\").cast(pl.Float64),\n",
        "    pl.col(\"func\").cast(pl.Utf8)\n",
        "])\n",
        "\n",
        "\n",
        "for capa in capas:\n",
        "  for neurona in neuronas:\n",
        "    for umbral in umbrales:\n",
        "      for lr in learning_rates:\n",
        "        for mejor_n_components in isomap_comp:\n",
        "          x_campos = campos_func(mejor_n_components)\n",
        "          for epoch_lim in epochs:\n",
        "            for f in func_out:\n",
        "              for seed in seeds:\n",
        "                if check_iter_list and (seed, capa, neurona, umbral, lr, epoch_lim, mejor_n_components) in list_of_iters:\n",
        "                  print(\"Ya se ha corrido este ejemplo, pass\")\n",
        "                else:\n",
        "                  contador += 1\n",
        "                  # Train test\n",
        "                  random.seed(seed)\n",
        "                  train_caras, train_nombres, test_caras, test_nombres = separar_aleatorio(faces, names, 0.7)\n",
        "                  df_train, df_test = procesar_isomap(train_caras, train_nombres, test_caras, test_nombres, mejor_n_components, mejor_n_neighbors = 30)\n",
        "\n",
        "                  hidden_layers_sizes = [neurona] * capa\n",
        "                  hidden_layers_sizes.append(13)\n",
        "                  layers_func = ['logsig'] * (len(hidden_layers_sizes)) + [f]\n",
        "                  print(layers_func)\n",
        "\n",
        "                  # Entrenamiento\n",
        "\n",
        "                  # defino la red multiperceptron\n",
        "                  carpeta = \"/content/buckets/b1/nn\" + str(contador) + \"/\"  # cambiar con cada corrida\n",
        "                  semilla_red = seed  # define las rectas iniciales\n",
        "\n",
        "                  mp = multiperceptron()\n",
        "                  mp.inicializar(\n",
        "                      df=df_train, campos = x_campos, clase=\"y\",  # especificaion del dataset\n",
        "                      hidden_layers_sizes = hidden_layers_sizes,  # no va la capa final, solo hidden layers\n",
        "                      layers_func=layers_func, # funciones de activacion de cada capa\n",
        "                      semilla=semilla_red,\n",
        "                      carpeta = carpeta\n",
        "                      )\n",
        "\n",
        "                  (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                    epoch_limit=epoch_lim,\n",
        "                    MSE_umbral=umbral,\n",
        "                    learning_rate=lr,\n",
        "                    lr_momento=0.2,\n",
        "                    save_frequency=100,\n",
        "                    retomar=True)\n",
        "\n",
        "                  # Predicción\n",
        "                  (y_hat,y_raw, test_error_rate) = mp.predecir(df_new=df_test, campos=x_campos, clase='y')\n",
        "\n",
        "                  new_row = pl.DataFrame({'seed': [seed], 'capa': [capa], 'neurona': [neurona], 'umbral': [umbral], 'lr': [lr], 'epoch': [epoch], 'epoch_lim': [epoch_lim], 'componente': [mejor_n_components], 'error_rate': [test_error_rate], 'func':[f] })\n",
        "                  print(new_row)\n",
        "                  df_results = pl.concat([df_results, new_row])\n",
        "\n",
        "fin = datetime.datetime.now()\n",
        "print(fin - inicio)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_in = df_results"
      ],
      "metadata": {
        "id": "oZSky2o1W_aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI7VoVUrFuEW"
      },
      "outputs": [],
      "source": [
        "## Test reciclado de epochs\n",
        "\n",
        "#inicio2 = datetime.datetime.now()\n",
        "seeds = [200177, 1999, 1996, 1949] # seeds a utilizar\n",
        "learning_rates = [0.2] #learning rates a probar\n",
        "epochs = [100, 500, 700, 1000, 1500,2000] #Límites de epochs a usar, PONER EN ORDEN CRECIENTE\n",
        "umbrales = [0.0001] # umbrales de mse a probar\n",
        "capas = [1, 2] # capas a usar, sin contar la última oculta, o sea, la red va a tener capa[i] + 1 (verificamos que usar una más overfitea mucho)\n",
        "neuronas = [20,25,30] # Neuronas a usar en las capas declaradas anteriormente, por ahora dejamos fija la última oculta con 13 (a revisar)\n",
        "isomap_comp = [70, 80] # n_components de isomap a probar, por ahora mantenemos el número de vecinos fijos\n",
        "contador = 0 #Contador solamente para guardar los diferentes pkl de las redes\n",
        "check_iter_list = 'list_of_iters' in locals()\n",
        "func_out = ['logsig', 'tansig']\n",
        "\n",
        "#para test\n",
        "# seeds = [200177] # seeds a utilizar\n",
        "# learning_rates = [0.2] #learning rates a probar\n",
        "# epochs = [100, 500, 700, 1000, 1500] #Límites de epochs a usar, PONER EN ORDEN CRECIENTE\n",
        "# umbrales = [0.0001] # umbrales de mse a probar\n",
        "# capas = [1] # capas a usar, sin contar la última oculta, o sea, la red va a tener capa[i] + 1 (verificamos que usar una más overfitea mucho)\n",
        "# neuronas = [20] # Neuronas a usar en las capas declaradas anteriormente, por ahora dejamos fija la última oculta con 13 (a revisar)\n",
        "# isomap_comp = [80] # n_components de isomap a probar, por ahora mantenemos el número de vecinos fijos\n",
        "# contador = 0 #Contador solamente para guardar los diferentes pkl de las redes\n",
        "# check_iter_list = 'list_of_iters' in locals()\n",
        "# func_out = ['logsig']\n",
        "\n",
        "# Creamos las listas de nombres y caras para usar en la red\n",
        "faces = [sublista[0] for sublista in caras]\n",
        "names = [sublista[1] for sublista in caras]\n",
        "\n",
        "# Creación del df para guardar los resultados que vamos obteniendo (y no morir en el intento)\n",
        "# seeds <- epochs <- lr <- umbral <- neuronas <- capas (ToDo: revisar para reciclar epochs y particiones de seed, ahora está haciendo uno por cada iter)\n",
        "\n",
        "data = {'seed': [], 'capa': [], 'neurona': [], 'umbral': [], 'lr': [], 'epoch': [], 'epoch_lim': [],  'componente': [], 'error_rate': [], 'func': []}\n",
        "df_results = pl.DataFrame(data)\n",
        "\n",
        "df_results = df_results.with_columns([\n",
        "    pl.col(\"epoch\").cast(pl.Int64),\n",
        "    pl.col(\"epoch_lim\").cast(pl.Int64),\n",
        "    pl.col(\"seed\").cast(pl.Int64),\n",
        "    pl.col(\"capa\").cast(pl.Int64),\n",
        "    pl.col(\"neurona\").cast(pl.Int64),\n",
        "    pl.col(\"componente\").cast(pl.Int64),\n",
        "    pl.col(\"error_rate\").cast(pl.Float64),\n",
        "    pl.col(\"umbral\").cast(pl.Float64),\n",
        "    pl.col(\"lr\").cast(pl.Float64),\n",
        "    pl.col(\"func\").cast(pl.Utf8)\n",
        "])\n",
        "\n",
        "\n",
        "for capa in capas:\n",
        "  for neurona in neuronas:\n",
        "    for umbral in umbrales:\n",
        "      for lr in learning_rates:\n",
        "        for mejor_n_components in isomap_comp:\n",
        "          x_campos = campos_func(mejor_n_components)\n",
        "          for seed in seeds:\n",
        "            for f in func_out:\n",
        "              contador += 1\n",
        "              epoch_init = True\n",
        "              skip = False\n",
        "              for epoch_lim in epochs:\n",
        "                if check_iter_list and (seed, capa, neurona, umbral, lr, epoch_lim, mejor_n_components) in list_of_iters:\n",
        "                  print(\"Ya se ha corrido este ejemplo, pass\")\n",
        "                else:\n",
        "                  # Train test\n",
        "                  random.seed(seed)\n",
        "                  train_caras, train_nombres, test_caras, test_nombres = separar_aleatorio(faces, names, 0.7)\n",
        "                  df_train, df_test = procesar_isomap(train_caras, train_nombres, test_caras, test_nombres, mejor_n_components, mejor_n_neighbors = 5)\n",
        "\n",
        "                  hidden_layers_sizes = [neurona] * capa\n",
        "                  hidden_layers_sizes.append(13)\n",
        "                  layers_func = ['logsig'] * (len(hidden_layers_sizes)) + [f]\n",
        "                  print(layers_func)\n",
        "\n",
        "                  # Entrenamiento\n",
        "\n",
        "                  # defino la red multiperceptron\n",
        "                  carpeta = \"/content/buckets/b1/nn\" + str(contador)  # cambiar con cada corrida\n",
        "                  semilla_red = seed  # define las rectas iniciales\n",
        "\n",
        "                  if epoch_init:\n",
        "                    epoch_init = False\n",
        "                    mp = multiperceptron()\n",
        "                    mp.inicializar(\n",
        "                        df=df_train, campos = x_campos, clase=\"y\",  # especificaion del dataset\n",
        "                        hidden_layers_sizes = hidden_layers_sizes,  # no va la capa final, solo hidden layers\n",
        "                        layers_func=layers_func, # funciones de activacion de cada capa\n",
        "                        semilla=semilla_red,\n",
        "                        carpeta = carpeta\n",
        "                        )\n",
        "\n",
        "                    (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                      epoch_limit=epoch_lim,\n",
        "                      MSE_umbral=umbral,\n",
        "                      learning_rate=lr,\n",
        "                      lr_momento=0.2,\n",
        "                      save_frequency=100,\n",
        "                      retomar=True)\n",
        "\n",
        "                    if epoch < epoch_lim:  #Si llega al limite antes no tiene sentido seguir corriendo epochs\n",
        "                      skip = True\n",
        "                  else: #reciclo el anterior\n",
        "                    if not skip:\n",
        "                      print('Hi')\n",
        "                      new_lim = epoch_lim - epoch # el aumento en epochs\n",
        "                      (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                        epoch_limit = epoch_lim, # aumento\n",
        "                        MSE_umbral=umbral,\n",
        "                        learning_rate=lr,\n",
        "                        lr_momento=0.2,\n",
        "                        save_frequency=100,\n",
        "                        retomar=True)\n",
        "\n",
        "                      if epoch < epoch_lim:\n",
        "                        skip = True\n",
        "\n",
        "\n",
        "                    else:\n",
        "                      print('no')\n",
        "\n",
        "                  # Predicción\n",
        "                  (y_hat,y_raw, test_error_rate) = mp.predecir(df_new=df_test, campos=x_campos, clase='y')\n",
        "\n",
        "                  new_row = pl.DataFrame({'seed': [seed], 'capa': [capa], 'neurona': [neurona], 'umbral': [umbral], 'lr': [lr], 'epoch': [epoch], 'epoch_lim': [epoch_lim], 'componente': [mejor_n_components], 'error_rate': [test_error_rate], 'func':[f] })\n",
        "                  print(new_row)\n",
        "                  df_results = pl.concat([df_results, new_row])\n",
        "                  print(contador)\n",
        "\n",
        "\n",
        "#fin2 = datetime.datetime.now()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results"
      ],
      "metadata": {
        "id": "Rm35vF-5J3C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a code to summarize the df_result df. It has to do a mean of the error_rate grouping by capa1, capa2, umbral, lr, epoch_lim and componente\n",
        "\n",
        "df_summary = df_results.group_by([\"capa\", \"neurona\",\"umbral\", \"lr\", \"epoch_lim\", \"componente\", \"func\"]).agg(pl.col(\"error_rate\").mean())\n",
        "df_summary\n"
      ],
      "metadata": {
        "id": "g2EjhKRn5HWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary.write_csv('/content/buckets/b1/DMA/NuestrasCaras/corridasResumen100x100.csv')"
      ],
      "metadata": {
        "id": "32qfD2iF5asg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test reciclado de epochs 2 capas\n",
        "\n",
        "#inicio2 = datetime.datetime.now()\n",
        "seeds = [200177, 1999, 1996, 1949] # seeds a utilizar\n",
        "learning_rates = [0.2] #learning rates a probar\n",
        "epochs = [100, 300, 500, 700, 1000, 1500, 2000, 2300, 2500] #Límites de epochs a usar, PONER EN ORDEN CRECIENTE\n",
        "umbrales = [0.0001] # umbrales de mse a probar\n",
        "capa1 = [13, 15, 20, 25, 30] # capas a usar, sin contar la última oculta, o sea, la red va a tener capa[i] + 1 (verificamos que usar una más overfitea mucho)\n",
        "capa2 = [13, 15, 20, 25, 30] # Neuronas a usar en las capas declaradas anteriormente, por ahora dejamos fija la última oculta con 13 (a revisar)\n",
        "isomap_comp = [70, 80] # n_components de isomap a probar, por ahora mantenemos el número de vecinos fijos\n",
        "contador = 0 #Contador solamente para guardar los diferentes pkl de las redes\n",
        "check_iter_list = 'list_of_iters' in locals()\n",
        "func_out = ['logsig']\n",
        "\n",
        "# Creamos las listas de nombres y caras para usar en la red\n",
        "faces = [sublista[0] for sublista in caras]\n",
        "names = [sublista[1] for sublista in caras]\n",
        "\n",
        "# Creación del df para guardar los resultados que vamos obteniendo (y no morir en el intento)\n",
        "# seeds <- epochs <- lr <- umbral <- neuronas <- capas (ToDo: revisar para reciclar epochs y particiones de seed, ahora está haciendo uno por cada iter)\n",
        "\n",
        "data = {'seed': [], 'capa1': [], 'capa2': [], 'umbral': [], 'lr': [], 'epoch': [], 'epoch_lim': [],  'componente': [], 'error_rate': [], 'func': []}\n",
        "df_results = pl.DataFrame(data)\n",
        "\n",
        "df_results = df_results.with_columns([\n",
        "    pl.col(\"epoch\").cast(pl.Int64),\n",
        "    pl.col(\"epoch_lim\").cast(pl.Int64),\n",
        "    pl.col(\"seed\").cast(pl.Int64),\n",
        "    pl.col(\"capa1\").cast(pl.Int64),\n",
        "    pl.col(\"capa2\").cast(pl.Int64),\n",
        "    pl.col(\"componente\").cast(pl.Int64),\n",
        "    pl.col(\"error_rate\").cast(pl.Float64),\n",
        "    pl.col(\"umbral\").cast(pl.Float64),\n",
        "    pl.col(\"lr\").cast(pl.Float64),\n",
        "    pl.col(\"func\").cast(pl.Utf8)\n",
        "])\n",
        "\n",
        "\n",
        "for n1 in capa1:\n",
        "  for n2 in capa2:\n",
        "    for umbral in umbrales:\n",
        "      for lr in learning_rates:\n",
        "        for mejor_n_components in isomap_comp:\n",
        "          x_campos = campos_func(mejor_n_components)\n",
        "          for seed in seeds:\n",
        "            for f in func_out:\n",
        "              contador += 1\n",
        "              epoch_init = True\n",
        "              skip = False\n",
        "              for epoch_lim in epochs:\n",
        "                # Train test\n",
        "                random.seed(seed)\n",
        "                train_caras, train_nombres, test_caras, test_nombres = separar_aleatorio(faces, names, 0.7)\n",
        "                df_train, df_test = procesar_isomap(train_caras, train_nombres, test_caras, test_nombres, mejor_n_components, mejor_n_neighbors = 5)\n",
        "\n",
        "                hidden_layers_sizes = [n1, n2]\n",
        "                layers_func = ['logsig'] * (len(hidden_layers_sizes)) + [f]\n",
        "                print(layers_func)\n",
        "\n",
        "                # Entrenamiento\n",
        "\n",
        "                # defino la red multiperceptron\n",
        "                carpeta = \"/content/buckets/b1/nn\" + str(contador)  # cambiar con cada corrida\n",
        "                semilla_red = seed  # define las rectas iniciales\n",
        "\n",
        "                if epoch_init:\n",
        "                  epoch_init = False\n",
        "                  mp = multiperceptron()\n",
        "                  mp.inicializar(\n",
        "                      df=df_train, campos = x_campos, clase=\"y\",  # especificaion del dataset\n",
        "                      hidden_layers_sizes = hidden_layers_sizes,  # no va la capa final, solo hidden layers\n",
        "                      layers_func=layers_func, # funciones de activacion de cada capa\n",
        "                      semilla=semilla_red,\n",
        "                      carpeta = carpeta\n",
        "                      )\n",
        "\n",
        "                  (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                    epoch_limit=epoch_lim,\n",
        "                    MSE_umbral=umbral,\n",
        "                    learning_rate=lr,\n",
        "                    lr_momento=0.2,\n",
        "                    save_frequency=100,\n",
        "                    retomar=True)\n",
        "\n",
        "                  if epoch < epoch_lim:  #Si llega al limite antes no tiene sentido seguir corriendo epochs\n",
        "                    skip = True\n",
        "                else: #reciclo el anterior\n",
        "                  if not skip:\n",
        "                    print('Hi')\n",
        "                    new_lim = epoch_lim - epoch # el aumento en epochs\n",
        "                    (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                      epoch_limit = epoch_lim, # aumento\n",
        "                      MSE_umbral=umbral,\n",
        "                      learning_rate=lr,\n",
        "                      lr_momento=0.2,\n",
        "                      save_frequency=100,\n",
        "                      retomar=True)\n",
        "\n",
        "                    if epoch < epoch_lim:\n",
        "                      skip = True\n",
        "\n",
        "\n",
        "                  else:\n",
        "                    print('no')\n",
        "\n",
        "                # Predicción\n",
        "                (y_hat,y_raw, test_error_rate) = mp.predecir(df_new=df_test, campos=x_campos, clase='y')\n",
        "\n",
        "                new_row = pl.DataFrame({'seed': [seed], 'capa1': [n1], 'capa2': [n2], 'umbral': [umbral], 'lr': [lr], 'epoch': [epoch], 'epoch_lim': [epoch_lim], 'componente': [mejor_n_components], 'error_rate': [test_error_rate], 'func':[f] })\n",
        "                print(new_row)\n",
        "                df_results = pl.concat([df_results, new_row])\n",
        "                print(contador)\n",
        "\n",
        "\n",
        "#fin2 = datetime.datetime.now()"
      ],
      "metadata": {
        "id": "FVC6gJfqtRot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary2 = df_results.group_by([\"capa1\", \"capa2\",\"umbral\", \"lr\", \"epoch_lim\", \"componente\", \"func\"]).agg(pl.col(\"error_rate\").mean())\n",
        "df_summary2"
      ],
      "metadata": {
        "id": "3_THcQZIu2a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary2.write_csv('/content/buckets/b1/DMA/NuestrasCaras/corridasResumen100x100Final.csv')"
      ],
      "metadata": {
        "id": "afR2T_8Bu7e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_in"
      ],
      "metadata": {
        "id": "AEijp-z4J5Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lfOet20mOl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnjPw6Ojbu5l"
      },
      "outputs": [],
      "source": [
        "df_results.write_csv('/content/buckets/b1/DMA/NuestrasCaras/corridasult.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsbB1ORx-DlP"
      },
      "outputs": [],
      "source": [
        "df_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbJquqavNDLu"
      },
      "source": [
        "#Test caras externas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "fotos_prueba_path = '/content/drive/MyDrive/Fotos_Nuestras'  # Reemplaza con la ruta correcta"
      ],
      "metadata": {
        "id": "j5AaMXJOTzqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#Importar caras\n",
        "def importar_fotos(path):\n",
        "  '''\n",
        "  Puedes acceder a cada foto y su nombre de carpeta así:\n",
        "  for foto, nombre_carpeta in lista_fotos:\n",
        "   print(f\"Foto: {foto}, Carpeta: {nombre_carpeta}\")\n",
        "  '''\n",
        "  lista_fotos = []\n",
        "  for carpeta in os.listdir(fotos_prueba_path):\n",
        "    carpeta_path = os.path.join(fotos_prueba_path, carpeta)\n",
        "    if os.path.isdir(carpeta_path):\n",
        "      for archivo in os.listdir(carpeta_path):\n",
        "        archivo_path = os.path.join(carpeta_path, archivo)\n",
        "        if os.path.isfile(archivo_path):\n",
        "          try:\n",
        "            img = cv2.imread(archivo_path, cv2.IMREAD_GRAYSCALE) # Importamos en gris para ahorrar memoria\n",
        "            lista_fotos.append([img, carpeta])\n",
        "            del img #para ir ganando puchitos de memoria, en caso de que sean más se puede unir al paso de detección\n",
        "          except Exception as e:\n",
        "            print(f\"Error al abrir la imagen {archivo_path}: {e}\")\n",
        "\n",
        "  print(f\"Se encontraron {len(lista_fotos)} fotos.\")\n",
        "  return lista_fotos\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wl-6H38GSN9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagenes = importar_fotos(fotos_prueba_path)"
      ],
      "metadata": {
        "id": "bwYwLYpwT2VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga de Archivos\n",
        "!wget -q https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt -O deploy.prototxt\n",
        "!wget -q https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel -O res10_300x300_ssd_iter_140000.caffemodel"
      ],
      "metadata": {
        "id": "sGiMKCy9mIgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DNN Face Detector\n",
        "face_net = cv2.dnn.readNetFromCaffe(\n",
        "    'deploy.prototxt',\n",
        "    'res10_300x300_ssd_iter_140000.caffemodel'\n",
        ")\n",
        "\n",
        "\n",
        "# Resultados\n",
        "caras_test = []\n",
        "errores_test = []\n",
        "\n",
        "# ✅ Paso 3: Iterar sobre las imágenes\n",
        "# imagenes = [...]  # <- asegurate que sea una lista de [imagen_array, etiqueta]\n",
        "flag = 0\n",
        "\n",
        "for imagen in imagenes:\n",
        "    flag += 1\n",
        "    print(f\"Procesando imagen {flag}\")\n",
        "\n",
        "    img = imagen[0].copy()\n",
        "    imagen[0] = np.array([]) #Para ir ahorrando memoria\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # Paso 1: Detectar cara\n",
        "    blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300), (104, 177, 123))\n",
        "    face_net.setInput(blob)\n",
        "    detections = face_net.forward()\n",
        "\n",
        "    found = False\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > 0.5:\n",
        "            found = True\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            x1, y1, x2, y2 = box.astype(\"int\")\n",
        "            x1, y1, x2, y2 = max(x1, 0), max(y1, 0), min(x2, w), min(y2, h)\n",
        "\n",
        "            cara = img[y1:y2, x1:x2]\n",
        "            if cara.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Paso 3: Guardar rostro recortado (usamos FaceMesh si querés más info)\n",
        "            rostro = cv2.resize(cara, (50, 50))\n",
        "            caras_test.append([rostro, imagen[1]])\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        errores_test.append([img, imagen[1]])"
      ],
      "metadata": {
        "id": "wEzmMzOmmKXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cara in caras_test:\n",
        "  cv2_imshow(cara[0])"
      ],
      "metadata": {
        "id": "zFPFh_8fdPjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(caras_test)):\n",
        "  caras_test[i][0] = cv2.cvtColor(caras_test[i][0], cv2.COLOR_BGR2GRAY).reshape(1, 2500)"
      ],
      "metadata": {
        "id": "LCDpY06rUkU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isomap\n",
        "mejor_n_components = 80\n",
        "# Creamos las listas de nombres y caras para usar en la red\n",
        "train_caras = [sublista[0] for sublista in caras]\n",
        "train_nombres = [sublista[1] for sublista in caras]\n",
        "test_caras = [sublista[0] for sublista in caras_test]\n",
        "test_nombres = [sublista[1] for sublista in caras_test]\n",
        "\n",
        "df_train, df_test = procesar_isomap(train_caras, train_nombres, test_caras, test_nombres, mejor_n_components, mejor_n_neighbors = 10)"
      ],
      "metadata": {
        "id": "rPV-99paVZ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfT3DkHXNK49"
      },
      "outputs": [],
      "source": [
        "# Test para caras externas\n",
        "\n",
        "\n",
        "## Test reciclado de epochs 2 capas\n",
        "\n",
        "#inicio2 = datetime.datetime.now()\n",
        "seeds = [200177, 19999] # seeds a utilizar\n",
        "learning_rates = [0.2, 0.05] #learning rates a probar\n",
        "epochs = [100, 300, 500, 700, 1000, 1500, 2000] #Límites de epochs a usar, PONER EN ORDEN CRECIENTE\n",
        "umbrales = [0.0001] # umbrales de mse a probar\n",
        "capa1 = [20, 25, 30, 35, 40] # capas a usar, sin contar la última oculta, o sea, la red va a tener capa[i] + 1 (verificamos que usar una más overfitea mucho)\n",
        "capa2 = [20, 25, 30, 35, 40]  # Neuronas a usar en las capas declaradas anteriormente, por ahora dejamos fija la última oculta con 13 (a revisar)\n",
        "isomap_comp = [80] # n_components de isomap a probar, por ahora mantenemos el número de vecinos fijos\n",
        "contador = 0 #Contador solamente para guardar los diferentes pkl de las redes\n",
        "check_iter_list = 'list_of_iters' in locals()\n",
        "func_out = ['logsig']\n",
        "\n",
        "# Creamos las listas de nombres y caras para usar en la red\n",
        "faces = [sublista[0] for sublista in caras]\n",
        "names = [sublista[1] for sublista in caras]\n",
        "\n",
        "# ToDo armar df_train\n",
        "\n",
        "# Creación del df para guardar los resultados que vamos obteniendo (y no morir en el intento)\n",
        "# seeds <- epochs <- lr <- umbral <- neuronas <- capas (ToDo: revisar para reciclar epochs y particiones de seed, ahora está haciendo uno por cada iter)\n",
        "\n",
        "data = {'seed': [], 'capa1': [], 'capa2': [], 'umbral': [], 'lr': [], 'epoch': [], 'epoch_lim': [],  'componente': [], 'MSE': [], 'error_rate': [], 'func': []}\n",
        "df_results = pl.DataFrame(data)\n",
        "\n",
        "df_results = df_results.with_columns([\n",
        "    pl.col(\"epoch\").cast(pl.Int64),\n",
        "    pl.col(\"epoch_lim\").cast(pl.Int64),\n",
        "    pl.col(\"seed\").cast(pl.Int64),\n",
        "    pl.col(\"capa1\").cast(pl.Int64),\n",
        "    pl.col(\"capa2\").cast(pl.Int64),\n",
        "    pl.col(\"componente\").cast(pl.Int64),\n",
        "    pl.col(\"error_rate\").cast(pl.Float64),\n",
        "    pl.col(\"MSE\").cast(pl.Float64),\n",
        "    pl.col(\"umbral\").cast(pl.Float64),\n",
        "    pl.col(\"lr\").cast(pl.Float64),\n",
        "    pl.col(\"func\").cast(pl.Utf8)\n",
        "])\n",
        "\n",
        "\n",
        "for n1 in capa1:\n",
        "  for n2 in capa2:\n",
        "    for umbral in umbrales:\n",
        "      for lr in learning_rates:\n",
        "        for mejor_n_components in isomap_comp:\n",
        "          x_campos = campos_func(mejor_n_components)\n",
        "          for seed in seeds:\n",
        "            for f in func_out:\n",
        "              contador += 1\n",
        "              epoch_init = True\n",
        "              skip = False\n",
        "              for epoch_lim in epochs:\n",
        "                # Train test\n",
        "\n",
        "                hidden_layers_sizes = [n1, n2]\n",
        "                layers_func = ['logsig'] * (len(hidden_layers_sizes)) + [f]\n",
        "                print(layers_func)\n",
        "\n",
        "                # Entrenamiento\n",
        "\n",
        "                # defino la red multiperceptron\n",
        "                carpeta = \"/content/buckets/b1/nn\" + str(contador)  # cambiar con cada corrida\n",
        "                semilla_red = seed  # define las rectas iniciales\n",
        "\n",
        "                if epoch_init:\n",
        "                  epoch_init = False\n",
        "                  mp = multiperceptron()\n",
        "                  mp.inicializar(\n",
        "                      df=df_train, campos = x_campos, clase=\"y\",  # especificaion del dataset\n",
        "                      hidden_layers_sizes = hidden_layers_sizes,  # no va la capa final, solo hidden layers\n",
        "                      layers_func=layers_func, # funciones de activacion de cada capa\n",
        "                      semilla=semilla_red,\n",
        "                      carpeta = carpeta\n",
        "                      )\n",
        "\n",
        "                  (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                    epoch_limit=epoch_lim,\n",
        "                    MSE_umbral=umbral,\n",
        "                    learning_rate=lr,\n",
        "                    lr_momento=0.2,\n",
        "                    save_frequency=100,\n",
        "                    retomar=True)\n",
        "\n",
        "                  if epoch < epoch_lim:  #Si llega al limite antes no tiene sentido seguir corriendo epochs\n",
        "                    skip = True\n",
        "                else: #reciclo el anterior\n",
        "                  if not skip:\n",
        "                    print('Hi')\n",
        "                    new_lim = epoch_lim - epoch # el aumento en epochs\n",
        "                    (epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "                      epoch_limit = epoch_lim, # aumento\n",
        "                      MSE_umbral=umbral,\n",
        "                      learning_rate=lr,\n",
        "                      lr_momento=0.2,\n",
        "                      save_frequency=100,\n",
        "                      retomar=True)\n",
        "\n",
        "                    if epoch < epoch_lim:\n",
        "                      skip = True\n",
        "\n",
        "\n",
        "                  else:\n",
        "                    print('no')\n",
        "\n",
        "                # Predicción\n",
        "                (y_hat,y_raw, test_error_rate) = mp.predecir(df_new=df_test, campos=x_campos, clase='y')\n",
        "\n",
        "                new_row = pl.DataFrame({'seed': [seed], 'capa1': [n1], 'capa2': [n2], 'umbral': [umbral], 'lr': [lr], 'epoch': [epoch], 'epoch_lim': [epoch_lim], 'componente': [mejor_n_components],'MSE':[MSE], 'error_rate': [test_error_rate],  'func':[f] })\n",
        "                print(new_row)\n",
        "                df_results = pl.concat([df_results, new_row])\n",
        "                print(contador)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "todo: pruebas con pocas neuronas por capas pero muchos epochs"
      ],
      "metadata": {
        "id": "ou9DaLuX2Ljt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpFjf3EUXhA9"
      },
      "outputs": [],
      "source": [
        "df_results.write_csv('/content/buckets/b1/DMA/NuestrasCaras/corridas_capas_50_fotosnati.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjrtUELnNcIL"
      },
      "outputs": [],
      "source": [
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.group_by([\"capa1\", \"capa2\",\"umbral\", \"lr\", \"epoch_lim\", \"componente\", \"func\"]).agg(pl.col(\"error_rate\").mean())"
      ],
      "metadata": {
        "id": "S5UPjjaZKz6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a code to summarize the df_result df. It has to do a mean of the error_rate grouping by capa1, capa2, umbral, lr, epoch_lim and componente\n",
        "\n",
        "df_summary = df_results.group_by([\"capa\", \"neurona\", , \"umbral\", \"lr\", \"epoch_lim\", \"componente\"]).agg(pl.col(\"error_rate\").mean())\n",
        "df_summary\n"
      ],
      "metadata": {
        "id": "jjx2JsknNqhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fotosnati\n",
        "\n",
        "capa1\tcapa2\tumbral\tlr\tepoch_lim\tcomponente\terror_rate\n",
        "\n",
        "30\t20\t0.0001\t0.2\t1000\t50\t0.18965008992779794   #logsig"
      ],
      "metadata": {
        "id": "gmCGDetoN9f0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1bqlJ2tANqAS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzAjl09ENJeh"
      },
      "source": [
        "fin del test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP3kiA1MH8dU"
      },
      "outputs": [],
      "source": [
        "# prompt: necesito un codigo que me abra un csv desde drive. El path está en mi drive pero que pueda abrirlo otra persona tambien. el enlace es: https://drive.google.com/file/d/18pouLFeG9hFI3N_DYKV9NiyCcfhbkGux/view?usp=drive_link . tiene que usar polars. It can be in another's person drive\n",
        "\n",
        "import polars as pl\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace with the actual path to your CSV file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/your_folder/your_file.csv'  # UPDATE THIS PATH\n",
        "\n",
        "try:\n",
        "  df = pl.read_csv(file_path)\n",
        "  print(df)\n",
        "except Exception as e:\n",
        "  print(f\"Error reading the CSV file: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZyuw5V4ySkc"
      },
      "source": [
        "Todo:\n",
        "\n",
        "\n",
        "*   Guardar MSE\n",
        "*   Cambiar for del seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt4VDBSb4rrn"
      },
      "source": [
        "### 2.2 Entrenamiento de la neural network = backpropagation\n",
        "\n",
        "Aqui se hace el trabajo pesado de entrenar la red neuronal\n",
        "\n",
        "Es necesario experimentar con\n",
        "\n",
        "\n",
        "*   learning_rate\n",
        "*   lr_momento\n",
        "*   epoch_limit  y MSE_umbral\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CvW7ZS8LoUt"
      },
      "outputs": [],
      "source": [
        "# entreno la neural netowork con BackPropagation\n",
        "\n",
        "# el entrenamiento\n",
        "(epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "    epoch_limit=20000,\n",
        "    MSE_umbral=0.0005,\n",
        "    learning_rate=0.2,\n",
        "    lr_momento=0.2,\n",
        "    save_frequency=100,\n",
        "    retomar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4X6DLWKA9Cv"
      },
      "source": [
        "#### Visualizacion de los resultados de la salida del entrenamiento de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY6RTcXO5vmj"
      },
      "outputs": [],
      "source": [
        "# las metricas basica de la red\n",
        "print(\"epoch :\", epoch)\n",
        "print(\"MSE :\", MSE)\n",
        "print(\"train_error_rate :\", train_error_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAlAN8M24i0c"
      },
      "outputs": [],
      "source": [
        "# la primera hidden layer\n",
        "print(\"W :\", mp.red[\"layer\"][0][\"W\"])\n",
        "print()\n",
        "print(\"w0 :\", mp.red[\"layer\"][0][\"w0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ZtSAAyRZJJ"
      },
      "source": [
        "### 2.3 Entrenamiento en caso de retomar\n",
        "\n",
        "\n",
        "\n",
        "*   Si se cortó el colab\n",
        "*   Si quiero extender la corrida a mas epochs\n",
        "*   Si quiero cambiar el learninh_rate\n",
        "*   Si quiero cambiar el MSE_umbral\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUthWw74RecV"
      },
      "outputs": [],
      "source": [
        "(epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "    epoch_limit=1800, # aumento\n",
        "    MSE_umbral=0.001,\n",
        "    learning_rate=0.05,\n",
        "    lr_momento=0.05,\n",
        "    save_frequency=100,\n",
        "    retomar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmVzHk1uBSCO"
      },
      "source": [
        "Visualizacion de los resultados de salida de un post entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JhOPrW7cHlQ"
      },
      "outputs": [],
      "source": [
        "print(\"epoch :\", epoch)\n",
        "print(\"MSE :\", MSE)\n",
        "print(\"train_error_rate :\", train_error_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkHzn8Mr5zb9"
      },
      "outputs": [],
      "source": [
        "# la primera hidden layer\n",
        "print(\"W :\", mp.red[\"layer\"][0][\"W\"])\n",
        "print()\n",
        "print(\"w0 :\", mp.red[\"layer\"][0][\"w0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6ApQFrX4wF5"
      },
      "source": [
        "## 3  Prediccion en los datos de Testing\n",
        "\n",
        "\n",
        "Se muestran los datos de testing, que son distintos a los de training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IscBRJaDdqS7"
      },
      "source": [
        "### 3.1 Prediccion en caliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmG4baz1kYtS"
      },
      "outputs": [],
      "source": [
        "# aplico la red entrenada al dataset de testing\n",
        "\n",
        "(y_hat,y_raw, test_error_rate) = mp.predecir(df_new=df_test, campos=campos, clase='y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ9Fm1dzBsNu"
      },
      "source": [
        "#### Visualizacion del error en testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTkTRyXt6CuO"
      },
      "outputs": [],
      "source": [
        "print(\"error_rate (train, test): \",  train_error_rate, test_error_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5NT12b4B1Vf"
      },
      "source": [
        "#### Visualizacion de la prediccion en testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnEcIAV7slNW"
      },
      "outputs": [],
      "source": [
        "tb_salida_test = pl.DataFrame( {\"clase\":df_test[\"y\"], \"pred\":y_hat, \"y_raw\":y_raw })\n",
        "tb_salida_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFSA6jf9f6Kx"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tb_salida_test)):\n",
        "    if not str(tb_salida_test[i][\"clase\"]) == str(tb_salida_test[i][\"pred\"]):\n",
        "      print(tb_salida_test[i][\"clase\"], tb_salida_test[i][\"pred\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8SvrJodiybt"
      },
      "source": [
        "#### Cálculo de errores por persona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyMxg7UshANc"
      },
      "outputs": [],
      "source": [
        "# Paso 1: crear una columna booleana que indique si acertó o no\n",
        "df = tb_salida_test.with_columns([\n",
        "    (pl.col(\"clase\") != pl.col(\"pred\")).alias(\"error\")\n",
        "])\n",
        "\n",
        "# Paso 2: agrupar por clase real y calcular tasa de error por clase\n",
        "errores_por_clase = (\n",
        "    df.group_by(\"clase\")\n",
        "    .agg([\n",
        "        pl.len().alias(\"total\"),\n",
        "        pl.col(\"error\").sum().alias(\"errores\")\n",
        "    ])\n",
        "    .with_columns([\n",
        "        (pl.col(\"errores\") / pl.col(\"total\")).alias(\"error_rate\")\n",
        "    ])\n",
        "    .sort(\"error_rate\", descending=True)\n",
        ")\n",
        "pl.Config.set_tbl_rows(20)\n",
        "print(errores_por_clase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A307crj1bE5G"
      },
      "outputs": [],
      "source": [
        "def analisis_fotos(fotos_reales, prediccion, fotos_error = True):\n",
        "  for i in range(len(prediccion)):\n",
        "    if ((fotos_error and prediccion['error'][i]) or (not(fotos_error) and not(prediccion['error'][i]))):\n",
        "      print(f\"La foto era de {prediccion['clase'][i]} y la red dijo que era de {prediccion['pred'][i]}\")\n",
        "      plt.figure(figsize=(1, 1))\n",
        "      plt.imshow(test_caras[i].reshape(30, 30), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpLfDrRhkHTl"
      },
      "outputs": [],
      "source": [
        "analisis_fotos(data_testing, df, True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vjspZVPdwQG"
      },
      "source": [
        "## 4 Prediccion en datos NUEVOS\n",
        "\n",
        "\n",
        "*   La red fue entrenada en el pasado, y se grabó al drive\n",
        "*   Ya no esta disponible la sesion donde se entreno\n",
        "*   No quiero volver a entrenar de cero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2htu2gdK7Clx"
      },
      "outputs": [],
      "source": [
        "# cargo datos NUEVOS\n",
        "df_new = pl.read_csv('https://storage.googleapis.com/open-courses/austral2025-af91/nuevos02.txt', separator='\\t')\n",
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7sLsqi9dzWc"
      },
      "outputs": [],
      "source": [
        "# cargo modelo grabado y lo aplico a los datos nuevos\n",
        "\n",
        "carpeta = \"/content/buckets/b1/nn01/\"  # la carpeta del modelo entrenado\n",
        "\n",
        "mp_frio = multiperceptron()\n",
        "(epoch, MSE, train_error_rate) = mp_frio.cargar_modelo(carpeta)\n",
        "\n",
        "(y_hat, y_raw, new_error_rate) = mp_frio.predecir(df_new=df_new, campos=['x1', 'x2'], clase='y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBEXEipeB-KW"
      },
      "source": [
        "#### Visualizacion del error modeloa aplicado a datos nuevos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Exvy_youeFLz"
      },
      "outputs": [],
      "source": [
        "print(\"error_rate (train, new): \",  train_error_rate, new_error_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6gJyIv1LGOa"
      },
      "source": [
        "#### Visualizacion de la prediccion en datos nuevos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YONa1F9-eHQ6"
      },
      "outputs": [],
      "source": [
        "tb_salida_new = pl.DataFrame( {\"clase\":df_new[\"y\"], \"pred\":y_hat, \"y_raw\":y_raw })\n",
        "tb_salida_new"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}