{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZAAQH_AnX1"
      },
      "source": [
        "# Algoritmo de BackPropagation multiclase\n",
        "\n",
        "Este codigo se utilizará para entrenar la red neuronal de clasificacion de Nuestras Caras\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3MCES9WMJEw"
      },
      "source": [
        "### Código en Python del Algoritmo de BackPropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsDfkqXaZR5q"
      },
      "source": [
        "La clase del dataset debe ser categorica\n",
        "Existe una clase en Python que resuelve el problema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0XkHjieK2xD"
      },
      "outputs": [],
      "source": [
        "# conexion al Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/.drive')\n",
        "!mkdir -p \"/content/.drive/My Drive/DMA\"\n",
        "!mkdir -p \"/content/buckets\"\n",
        "!ln -s \"/content/.drive/My Drive/DMA\" /content/buckets/b1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtiWbym5uwI6"
      },
      "outputs": [],
      "source": [
        "# instalo  itables solo si no esta instalado\n",
        "!pip show itables >/dev/null || pip install itables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejqYzmGGLADY"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "from functools import reduce\n",
        "from itables import init_notebook_mode\n",
        "init_notebook_mode(all_interactive=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bYjXyHqLQFe"
      },
      "outputs": [],
      "source": [
        "# definicion de la clase de graficos\n",
        "\n",
        "class perceptron_plot:\n",
        "    \"\"\"plotting first hidden layer class\"\"\"\n",
        "    def __init__(self, X, Y, delay) -> None:\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.delay = delay\n",
        "        x1_min = np.min(X[:,0])\n",
        "        x2_min = np.min(X[:,1])\n",
        "        x1_max = np.max(X[:,0])\n",
        "        x2_max = np.max(X[:,1])\n",
        "        self.x1_min = x1_min - 0.1*(x1_max - x1_min)\n",
        "        self.x1_max = x1_max + 0.1*(x1_max - x1_min)\n",
        "        self.x2_min = x2_min - 0.1*(x2_max - x2_min)\n",
        "        self.x2_max = x2_max + 0.1*(x2_max - x2_min)\n",
        "        self.fig = plt.figure(figsize = (10,8))\n",
        "        self.ax = self.fig.subplots()\n",
        "        self.ax.set_xlim(self.x1_min, self.x1_max, auto=False)\n",
        "        self.ax.set_ylim(self.x2_min, self.x2_max, auto=False)\n",
        "\n",
        "    def graficarVarias(self, W, x0, epoch, error) -> None:\n",
        "        display.clear_output(wait =True)\n",
        "        plt.cla()\n",
        "        #self.ax = self.fig.subplots()\n",
        "\n",
        "        self.ax.set_xlim(self.x1_min, self.x1_max)\n",
        "        self.ax.set_ylim(self.x2_min, self.x2_max)\n",
        "        plt.title( 'epoch ' + str(epoch) + '  reg ' + \"{0:.2E}\".format(error))\n",
        "        # ploteo puntos\n",
        "        num_classes = len(np.unique(self.Y))\n",
        "        # mycolors = plt.cm.get_cmap('tab10', num_classes)\n",
        "        unique_labels = np.unique(self.Y)\n",
        "        label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
        "        numerical_labels = np.array([label_mapping[label] for label in self.Y])\n",
        "        # scatter = self.ax.scatter(self.X[:,0], self.X[:,1], c=self.Y, s=20)\n",
        "        scatter = self.ax.scatter(self.X[:,0], self.X[:,1], c=numerical_labels, s=20, cmap=plt.get_cmap('viridis', num_classes))\n",
        "        # self.ax.plot(self.X[:,0], self.X[:,1], 'o', c=vcolores,  markersize=2)\n",
        "\n",
        "\n",
        "        # dibujo las rectas\n",
        "        for i in range(len(x0)):\n",
        "            #vx2_min = -(W[0,i]*self.x1_min + x0[i])/W[1,i]\n",
        "            #vx2_max = -(W[0,i]*self.x1_max + x0[i])/W[1,i]\n",
        "            vx2_min = -(W[i,0]*self.x1_min + x0[i])/W[i,1]\n",
        "            vx2_max = -(W[i,0]*self.x1_max + x0[i])/W[i,1]\n",
        "\n",
        "            self.ax.plot([self.x1_min, self.x1_max],\n",
        "                         [vx2_min, vx2_max],\n",
        "                         linewidth = 2,\n",
        "                         color = 'red',\n",
        "                         alpha = 0.5)\n",
        "\n",
        "        display.display(plt.gcf())\n",
        "        #plt.cla()\n",
        "        time.sleep(self.delay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DgbLgqe3gTp"
      },
      "outputs": [],
      "source": [
        "# definicion de las funciones de activacion\n",
        "#  y sus derivadas\n",
        "#  ahora agregando las versiones VECTORIZADAS\n",
        "\n",
        "def func_eval(fname, x):\n",
        "    match fname:\n",
        "        case \"purelin\":\n",
        "            y = x\n",
        "        case \"logsig\":\n",
        "            y = 1.0 / ( 1.0 + math.exp(-x) )\n",
        "        case \"tansig\":\n",
        "            y = 2.0 / ( 1.0 + math.exp(-2.0*x) ) - 1.0\n",
        "    return y\n",
        "\n",
        "# version vectorizada de func_eval\n",
        "func_eval_vec = np.vectorize(func_eval)\n",
        "\n",
        "\n",
        "def deriv_eval(fname, y):  #atencion que y es la entrada y=f( x )\n",
        "    match fname:\n",
        "        case \"purelin\":\n",
        "            d = 1.0\n",
        "        case \"logsig\":\n",
        "            d = y*(1.0-y)\n",
        "        case \"tansig\":\n",
        "            d = 1.0 - y*y\n",
        "    return d\n",
        "\n",
        "\n",
        "# version vectorizada de deriv_eval\n",
        "deriv_eval_vec = np.vectorize(deriv_eval)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: add to func_eval and deriv_eval the softmax function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def func_eval(fname, x):\n",
        "    match fname:\n",
        "        case \"purelin\":\n",
        "            y = x\n",
        "        case \"logsig\":\n",
        "            y = 1.0 / ( 1.0 + np.exp(-x) )\n",
        "        case \"tansig\":\n",
        "            y = 2.0 / ( 1.0 + np.exp(-2.0*x) ) - 1.0\n",
        "        case \"softmax\":  # Added softmax function\n",
        "            exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "            y = exp_x / np.sum(exp_x, axis=0)\n",
        "    return y\n",
        "\n",
        "# Vectorized version remains the same, as numpy handles vectorization for the new cases\n",
        "func_eval_vec = np.vectorize(func_eval)\n",
        "\n",
        "\n",
        "def deriv_eval(fname, y):\n",
        "    match fname:\n",
        "        case \"purelin\":\n",
        "            d = 1.0\n",
        "        case \"logsig\":\n",
        "            d = y * (1.0 - y)\n",
        "        case \"tansig\":\n",
        "            d = 1.0 - y * y\n",
        "        case \"softmax\": # Added softmax derivative (Note: this is the Jacobian matrix)\n",
        "            # Derivative of softmax is a matrix. This returns the diagonal entries\n",
        "            # for the case where you need the derivative of a single output element with\n",
        "            # respect to its input.  For full Jacobian, refer to the documentation.\n",
        "            d = np.diagflat(y) - np.outer(y, y)\n",
        "    return d\n",
        "\n",
        "# Vectorized version remains the same, as numpy handles vectorization for the new cases.\n",
        "deriv_eval_vec = np.vectorize(deriv_eval)\n"
      ],
      "metadata": {
        "id": "B8Z54R53QTRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXinnKQIFnxU"
      },
      "source": [
        "### Clase  multiperceptron\n",
        "entrenar, predecir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gv7esD65dqu"
      },
      "outputs": [],
      "source": [
        "# definicion de la clase de multiperceptron\n",
        "\n",
        "class multiperceptron(object):\n",
        "    \"\"\"Multiperceptron class\"\"\"\n",
        "\n",
        "    # inicializacion de los pesos de todas las capas\n",
        "    def _red_init(self, semilla) -> None:\n",
        "        niveles = self.red['arq']['layers_qty']\n",
        "\n",
        "        np.random.seed(semilla)\n",
        "        for i in range(niveles):\n",
        "           nivel = dict()\n",
        "           nivel['id'] = i\n",
        "           nivel['last'] = (i==(niveles-1))\n",
        "           nivel['size'] = self.red[\"arq\"][\"layers_size\"][i]\n",
        "           nivel['func'] = self.red[\"arq\"][\"layers_func\"][i]\n",
        "\n",
        "           if( i==0 ):\n",
        "              entrada_size = self.red['arq']['input_size']\n",
        "           else:\n",
        "              entrada_size =  self.red['arq']['layers_size'][i-1]\n",
        "\n",
        "           salida_size =  nivel['size']\n",
        "\n",
        "           # los pesos, inicializados random\n",
        "           nivel['W'] = np.random.uniform(-0.5, 0.5, [salida_size, entrada_size])\n",
        "           nivel['w0'] = np.random.uniform(-0.5, 0.5, [salida_size, 1])\n",
        "\n",
        "           # los momentos, inicializados en CERO\n",
        "           nivel['W_m'] = np.zeros([salida_size, entrada_size])\n",
        "           nivel['w0_m'] = np.zeros([salida_size, 1])\n",
        "\n",
        "           self.red['layer'].append(nivel)\n",
        "\n",
        "    # constructor generico\n",
        "    def __init__(self) -> None:\n",
        "        self.data = dict()\n",
        "        self.red = dict()\n",
        "        self.carpeta = \"\"\n",
        "\n",
        "\n",
        "    # inicializacion full\n",
        "    def inicializar(self, df, campos, clase, hidden_layers_sizes, layers_func,\n",
        "                 semilla, carpeta) -> None:\n",
        "\n",
        "        # genero self.data\n",
        "        self.data['X'] = np.array( df.select(campos))\n",
        "        X_mean = self.data['X'].mean(axis=0)\n",
        "        X_sd = self.data['X'].std(axis=0)\n",
        "        self.data['X'] = (self.data['X'] - X_mean)/X_sd\n",
        "\n",
        "        #  Ylabel en  numpy\n",
        "        label =df.select(clase)\n",
        "        self.data['Ylabel'] = np.array(label).reshape(len(label))\n",
        "\n",
        "        # one-hot-encoding de Y\n",
        "        col_originales = df.columns\n",
        "        self.data['Y'] = np.array( df.to_dummies(clase).drop(col_originales, strict=False) )\n",
        "\n",
        "        col_dummies = sorted( list( set(df.to_dummies(clase).columns) -  set(col_originales)))\n",
        "        clases_originales = reduce(lambda acc, x: acc + [x[(len(clase)+1):]], col_dummies, [])\n",
        "\n",
        "        tamanos = hidden_layers_sizes\n",
        "        tamanos.append(self.data['Y'].shape[1])\n",
        "\n",
        "        arquitectura = {\n",
        "             'input_size' : self.data['X'].shape[1],\n",
        "             'input_mean' : X_mean,\n",
        "             'input_sd' :  X_sd,\n",
        "             'output_values' : clases_originales,\n",
        "             'layers_qty' : len(hidden_layers_sizes), # incluye la capa de salida, pero no la de entrada\n",
        "             'layers_size' : tamanos ,\n",
        "             'layers_func' : layers_func,\n",
        "        }\n",
        "\n",
        "        self.red['arq'] = arquitectura\n",
        "\n",
        "\n",
        "        # inicializo  work\n",
        "        self.red['work'] = dict()\n",
        "        self.red['work']['epoch'] = 0\n",
        "        self.red['work']['MSE'] = float('inf')\n",
        "        self.red['work']['train_error_rate'] = float('inf')\n",
        "\n",
        "        self.red['layer'] = list()\n",
        "        self._red_init(semilla)\n",
        "\n",
        "        # grabo el entorno\n",
        "        self.carpeta = carpeta\n",
        "        os.makedirs(self.carpeta, exist_ok=True)\n",
        "        with open(self.carpeta+\"/data.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.data, f)\n",
        "\n",
        "        with open(self.carpeta+\"/red.pkl\", 'wb') as f:\n",
        "            pickle.dump(self.red, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Algoritmo Backpropagation\n",
        "    def  entrenar(self, epoch_limit, MSE_umbral,\n",
        "               learning_rate, lr_momento, save_frequency,\n",
        "               retomar=True) -> None:\n",
        "\n",
        "        # si debo retomar\n",
        "        if( retomar):\n",
        "            with open(self.carpeta+\"/data.pkl\", 'rb') as f:\n",
        "              self.data = pickle.load(f)\n",
        "\n",
        "            with open(self.carpeta+\"/red.pkl\", 'rb') as f:\n",
        "              self.red = pickle.load(f)\n",
        "\n",
        "\n",
        "        # inicializaciones del bucle principal del backpropagation\n",
        "        epoch = self.red['work']['epoch']\n",
        "        MSE = self.red['work']['MSE']\n",
        "\n",
        "        # inicializacion del grafico\n",
        "        grafico = perceptron_plot(X=self.data['X'], Y=self.data['Ylabel'], delay=0.1)\n",
        "\n",
        "        # continuo mientras error cuadratico medio muy grande  y NO llegué al límite de epochs\n",
        "        Xfilas = self.data['X'].shape[0]\n",
        "        niveles = self.red[\"arq\"][\"layers_qty\"]\n",
        "\n",
        "        while ( MSE > MSE_umbral) and (epoch < epoch_limit) :\n",
        "          epoch += 1\n",
        "\n",
        "\n",
        "          # recorro siempre TODOS los registros de entrada\n",
        "          for fila in range(Xfilas):\n",
        "             # fila es el registro actual\n",
        "             x = self.data['X'][fila:fila+1,:]\n",
        "             clase = self.data['Y'][fila:fila+1,:]\n",
        "\n",
        "             # propagar el x hacia adelante, FORWARD\n",
        "             entrada = x.T  # la entrada a la red\n",
        "\n",
        "             # etapa forward\n",
        "             # recorro hacia adelante, nivel a nivel\n",
        "             vsalida =  [0] *(niveles) # salida de cada nivel de la red\n",
        "\n",
        "             for i in range(niveles):\n",
        "               estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "               vsalida[i] =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "               entrada = vsalida[i]  # para la proxima vuelta\n",
        "\n",
        "\n",
        "             # etapa backward\n",
        "             # calculo los errores en la capa hidden y la capa output\n",
        "             verror =  [0] *(niveles+1) # inicializo dummy\n",
        "             verror[niveles] = clase.T - vsalida[niveles-1]\n",
        "\n",
        "             i = niveles-1\n",
        "             verror[i] = verror[i+1] * deriv_eval_vec(self.red['layer'][i]['func'], vsalida[i])\n",
        "\n",
        "             for i in reversed(range(niveles-1)):\n",
        "               verror[i] = deriv_eval_vec(self.red['layer'][i]['func'], vsalida[i])*(self.red['layer'][i+1]['W'].T @ verror[i+1])\n",
        "\n",
        "             # ya tengo los errores que comete cada capa\n",
        "             # corregir matrices de pesos, voy hacia atras\n",
        "             # backpropagation\n",
        "             entrada = x.T\n",
        "             for i in range(niveles):\n",
        "               self.red['layer'][i]['W_m'] = learning_rate *(verror[i] @ entrada.T) + lr_momento *self.red['layer'][i]['W_m']\n",
        "               self.red['layer'][i]['w0_m'] = learning_rate * verror[i] + lr_momento * self.red['layer'][i]['w0_m']\n",
        "\n",
        "               self.red['layer'][i]['W']  =  self.red['layer'][i]['W'] + self.red['layer'][i]['W_m']\n",
        "               self.red['layer'][i]['w0'] =  self.red['layer'][i]['w0'] + self.red['layer'][i]['w0_m']\n",
        "               entrada = vsalida[i]  # para la proxima vuelta\n",
        "\n",
        "\n",
        "\n",
        "          # ya recalcule las matrices de pesos\n",
        "          # ahora avanzo la red, feed-forward\n",
        "          # para calcular el red(X) = Y\n",
        "          entrada = self.data['X'].T\n",
        "          for i in range(niveles):\n",
        "            estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "            salida =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "            entrada = salida  # para la proxima vuelta\n",
        "\n",
        "          # calculo el error cuadratico medio TODOS los X del dataset\n",
        "          MSE= np.mean( (self.data['Y'].T - salida)**2 )\n",
        "\n",
        "          # Grafico las rectas SOLAMENTE de la Primera Hidden Layer\n",
        "          # tengo que hacer w0.T[0]  para que pase el vector limpio\n",
        "          if( epoch % save_frequency == 0 ) or ( MSE <= MSE_umbral) or (epoch >= epoch_limit) :\n",
        "              # grafico\n",
        "              W = self.red['layer'][0]['W']\n",
        "              w0 = self.red['layer'][0]['w0']\n",
        "              grafico.graficarVarias(W, w0.T[0], epoch, MSE)\n",
        "\n",
        "              # almaceno en work\n",
        "              self.red['work']['epoch'] = epoch\n",
        "              self.red['work']['MSE'] = MSE\n",
        "              prediccion = np.argmax( salida.T, axis=1)\n",
        "              # prediccion\n",
        "              out = np.array(self.red[\"arq\"]['output_values'])\n",
        "              error_rate = np.mean( self.data['Ylabel'] != out[prediccion])\n",
        "              self.red[\"work\"]['train_error_rate'] = error_rate # error_rate != error cuadratico medio\n",
        "\n",
        "              # grabo a un archivo la red neuronal  entrenada por donde esté\n",
        "              #   solo la red, NO los datos\n",
        "              with open(carpeta+\"/red.pkl\", 'wb') as f:\n",
        "                 pickle.dump(self.red, f)\n",
        "\n",
        "        return (epoch, MSE, self.red['work']['train_error_rate'] )\n",
        "\n",
        "\n",
        "    # predigo a partir de modelo recien entrenado\n",
        "    def  predecir(self, df_new, campos, clase) -> None:\n",
        "        niveles = self.red['arq']['layers_qty']\n",
        "\n",
        "        # etapa forward\n",
        "        # recorro hacia adelante, nivel a nivel\n",
        "        X_new =  np.array( df_new.select(campos))\n",
        "\n",
        "\n",
        "        # estandarizo manualmente\n",
        "        #  con las medias y desvios que almacene durante el entrenamiento\n",
        "        X_new = (X_new - self.red['arq']['input_mean'])/self.red['arq']['input_sd']\n",
        "\n",
        "        # grafico los datos nuevos\n",
        "        Ylabel_new =df_new.select(clase)\n",
        "        Ylabel_new = np.array(Ylabel_new).reshape(len(Ylabel_new))\n",
        "        grafico = perceptron_plot(X=X_new, Y=Ylabel_new, delay=0.1)\n",
        "        W = self.red['layer'][0]['W']\n",
        "        w0 = self.red['layer'][0]['w0']\n",
        "        grafico.graficarVarias(W, w0.T[0], epoch, MSE)\n",
        "\n",
        "        # la entrada a la red,  el X que es TODO  x_new\n",
        "        entrada = X_new.T  # traspongo, necesito vectores columna\n",
        "\n",
        "        for i in range(niveles):\n",
        "          estimulos = self.red['layer'][i]['W'] @ entrada + self.red['layer'][i]['w0']\n",
        "          salida =  func_eval_vec(self.red['layer'][i]['func'], estimulos)\n",
        "          entrada = salida  # para la proxima vuelta\n",
        "\n",
        "        # me quedo con la neurona de la ultima capa que se activio con mayor intensidad\n",
        "        pred_idx = np.argmax( salida.T, axis=1)\n",
        "        pred_raw = np.max( salida.T, axis=1)\n",
        "\n",
        "        # calculo error_rate\n",
        "        out = np.array(self.red['arq']['output_values'])\n",
        "        error_rate = np.mean( np.array(df_new.select(\"y\") != out[pred_idx]))\n",
        "\n",
        "        return (out[pred_idx], pred_raw, error_rate)\n",
        "\n",
        "\n",
        "    # cargo un modelo ya entrenado, grabado en carpeta\n",
        "    def cargar_modelo(self, carpeta) -> None:\n",
        "        self.carpeta = carpeta\n",
        "\n",
        "        with open(self.carpeta+\"/red.pkl\", 'rb') as f:\n",
        "          self.red = pickle.load(f)\n",
        "\n",
        "        return (self.red['work']['epoch'],\n",
        "                self.red['work']['MSE'],\n",
        "                self.red['work']['train_error_rate'] )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u3z27yD4aoF"
      },
      "source": [
        "## 1 Lectura del Dataset\n",
        "\n",
        "En este humilde y restringida version, la clase del dataset debe ser categorica, no es capaz de trabajar con clases continuas.\n",
        "La clase categorica puede ser  n-aria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZAoHm3HSwAQ"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/josekeh/dma_g2/main/input/caras50.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95kI-lzePHom"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"caras50.pkl\", \"rb\") as file:\n",
        "    caras = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HM107y_4gzg"
      },
      "source": [
        "### 1.1 Particion  training/testing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpEKQaxEX25a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Algoritmo para chequear la distribución de nombres\n",
        "def check_separar(training_nombres):\n",
        "  # Contar la frecuencia de cada palabra\n",
        "  contador = Counter(training_nombres)\n",
        "  total_palabras = sum(contador.values())\n",
        "  cantidad_diferentes = len(contador)  # Número de palabras únicas\n",
        "\n",
        "  # Convertir a porcentaje\n",
        "  frecuencias = {palabra: (conteo / total_palabras) * 100 for palabra, conteo in contador.items()}\n",
        "\n",
        "  umbral = 100 / cantidad_diferentes\n",
        "\n",
        "  # Mostrar el gráfico si no es muy dispar\n",
        "  if (min(frecuencias.values()) < umbral*.75 or max(frecuencias.values()) > umbral*1.25):\n",
        "    return 0\n",
        "  else:\n",
        "    # Graficar\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(frecuencias.keys(), frecuencias.values(), color='skyblue')\n",
        "\n",
        "    # Agregar línea horizontal en 100% / cantidad de palabras diferentes\n",
        "    plt.axhline(y=umbral, color='red', linestyle='--', label=f\"Ideal: {umbral:.2f}%\")\n",
        "\n",
        "    # Etiquetas\n",
        "    plt.xlabel(\"Persona\")\n",
        "    plt.ylabel(\"Frecuencia (%)\")\n",
        "    plt.title(\"Distribución del training set según las personas\")\n",
        "    plt.ylim(0, max(frecuencias.values()) + 5)  # Ajuste del eje Y\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "# Algoritmo para separar training y testing suponiendo 2 listas\n",
        "def separar_aleatorio(lista_caras, lista_nombres, porcentaje_training):\n",
        "    if (len(lista_caras) != len(lista_nombres)):\n",
        "      print(\"Error las listas deben tener la misma longitud\")\n",
        "    else:\n",
        "      chequeo = 0\n",
        "\n",
        "      while (chequeo == 0):\n",
        "        training_caras = []\n",
        "        training_nombres = []\n",
        "        testing_caras = []\n",
        "        testing_nombres = []\n",
        "\n",
        "        for i in range(len(lista_caras)):\n",
        "          aleatorio = random.random()\n",
        "          if aleatorio <= porcentaje_training:\n",
        "            training_caras.append(lista_caras[i])\n",
        "            training_nombres.append(lista_nombres[i])\n",
        "          else:\n",
        "            testing_caras.append(lista_caras[i])\n",
        "            testing_nombres.append(lista_nombres[i])\n",
        "        chequeo = check_separar(training_nombres)\n",
        "\n",
        "      return training_caras, training_nombres, testing_caras, testing_nombres\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGOnf47OYalP"
      },
      "outputs": [],
      "source": [
        "faces = [sublista[0] for sublista in caras]\n",
        "names = [sublista[1] for sublista in caras]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgJe2nyPYFIq"
      },
      "outputs": [],
      "source": [
        "random.seed(102191)\n",
        "train_caras, train_nombres, test_caras, test_nombres = separar_aleatorio(faces, names, 0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para la corrida final sin testing set"
      ],
      "metadata": {
        "id": "XnY47U6vuX_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faces = [sublista[0] for sublista in caras]\n",
        "names = [sublista[1] for sublista in caras]"
      ],
      "metadata": {
        "id": "e-uXnC-buhdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_caras = []\n",
        "test_nombres = []\n",
        "train_caras = []\n",
        "train_nombres = []\n",
        "for i in range(len(faces)):\n",
        "  if i in []:\n",
        "    test_caras.append(faces[i])\n",
        "    test_nombres.append(names[i])\n",
        "  else:\n",
        "    train_caras.append(faces[i])\n",
        "    train_nombres.append(names[i])"
      ],
      "metadata": {
        "id": "g2lPfO7qulTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_caras)):\n",
        "  print(test_nombres[i])\n",
        "  plt.figure(figsize=(1, 1))\n",
        "  plt.imshow(test_caras[i].reshape(50, 50), cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "YCXWTdgmxmAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy5Pg8D1TnYP"
      },
      "source": [
        "### ISOMAP\n",
        "\n",
        "Aclaración: En caso de estar trabajando con training y testing hay que \"descomentar\" el código comentado sobre el testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFQHAaAJTmue"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import Isomap\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI3XbHoZYx8i"
      },
      "outputs": [],
      "source": [
        "mejor_n_components = 35\n",
        "mejor_n_neighbors = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCUX8nzNP21V"
      },
      "outputs": [],
      "source": [
        "# Pasamos las caras de training y testing a listas\n",
        "data_training = []\n",
        "for j in range(len(train_caras)):\n",
        "  data_training.append(train_caras[j])\n",
        "\n",
        "data_testing = []\n",
        "for j in range(len(test_caras)):\n",
        "  data_testing.append(test_caras[j])\n",
        "\n",
        "# Convertimos las listas en matrices de tamaño (n, 900)\n",
        "data_training = np.vstack(data_training)\n",
        "#data_testing = np.vstack(data_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmkFgTy8Y7KO"
      },
      "outputs": [],
      "source": [
        "isomap = Isomap(n_components=mejor_n_components, n_neighbors=mejor_n_neighbors)\n",
        "output_isomap_training = isomap.fit_transform(data_training)\n",
        "\n",
        "#output_isomap_testing = isomap.transform(data_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhsBbaA9ZUWh"
      },
      "outputs": [],
      "source": [
        "df_train = pl.DataFrame(output_isomap_training)\n",
        "df_train.columns = ['x' + str(i + 1) for i in range(df_train.shape[1])]\n",
        "df_train = df_train.with_columns(pl.Series(train_nombres).alias(\"y\"))\n",
        "\n",
        "#df_test = pl.DataFrame(output_isomap_testing)\n",
        "#df_test.columns = ['x' + str(i + 1) for i in range(df_test.shape[1])]\n",
        "#df_test = df_test.with_columns(pl.Series(test_nombres).alias(\"y\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Código para exportar el isomap en un .pkl\n",
        "import pickle\n",
        "path = '/content/.drive/My Drive/DMA/NuestrasCaras/isomap.pkl'\n",
        "\n",
        "with open(path, 'wb') as f:\n",
        "    pickle.dump(isomap, f)"
      ],
      "metadata": {
        "id": "Rn3x6lxDmkd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j_YyhttTxA1"
      },
      "source": [
        "### Construcción de variable respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQE_x-XZnvQj"
      },
      "outputs": [],
      "source": [
        "campos = []\n",
        "for i in range(mejor_n_components):\n",
        "  campos.append(f\"x{i+1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVDBHdeF_Ai"
      },
      "source": [
        "## 2  Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH__fEUO4mED"
      },
      "source": [
        "### 2.1  Inicializacion de la neural network\n",
        "\n",
        "\n",
        "\n",
        "*   Es valido cambiar la *semilla_red* para arrancar el entrenamiento con distintas rectas iniciales\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FBNjEaQCXIH"
      },
      "outputs": [],
      "source": [
        "# defino la red multiperceptron\n",
        "carpeta = \"/content/buckets/b1/nn_prueba/\"  # cambiar con cada corrida\n",
        "semilla_red = 547389  # define las rectas iniciales\n",
        "\n",
        "# una sola capa oculta de 2 neuronas  [2]\n",
        "# la capa oculta y la final tienen ambas logsig de activacion\n",
        "mp = multiperceptron()\n",
        "mp.inicializar(\n",
        "    df=df_train, campos=campos, clase=\"y\",  # especificaion del dataset\n",
        "    hidden_layers_sizes=[30,20],  # no va la capa final, solo hidden layers\n",
        "    layers_func=['logsig','logsig','logsig'], # funciones de activacion de cada capa\n",
        "    semilla=semilla_red,\n",
        "    carpeta = carpeta\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt4VDBSb4rrn"
      },
      "source": [
        "### 2.2 Entrenamiento de la neural network = backpropagation\n",
        "\n",
        "Aqui se hace el trabajo pesado de entrenar la red neuronal\n",
        "\n",
        "Es necesario experimentar con\n",
        "\n",
        "\n",
        "*   learning_rate\n",
        "*   lr_momento\n",
        "*   epoch_limit  y MSE_umbral\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CvW7ZS8LoUt"
      },
      "outputs": [],
      "source": [
        "# entreno la neural netowork con BackPropagation\n",
        "\n",
        "# el entrenamiento\n",
        "(epoch, MSE, train_error_rate) = mp.entrenar(\n",
        "    epoch_limit=2500,\n",
        "    MSE_umbral=0.0001,\n",
        "    learning_rate=0.2,\n",
        "    lr_momento=0.2,\n",
        "    save_frequency=100,\n",
        "    retomar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4X6DLWKA9Cv"
      },
      "source": [
        "#### Visualizacion de los resultados de la salida del entrenamiento de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY6RTcXO5vmj"
      },
      "outputs": [],
      "source": [
        "# las metricas basica de la red\n",
        "print(\"epoch :\", epoch)\n",
        "print(\"MSE :\", MSE)\n",
        "print(\"train_error_rate :\", train_error_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAlAN8M24i0c"
      },
      "outputs": [],
      "source": [
        "# la primera hidden layer\n",
        "print(\"W :\", mp.red[\"layer\"][0][\"W\"])\n",
        "print()\n",
        "print(\"w0 :\", mp.red[\"layer\"][0][\"w0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6ApQFrX4wF5"
      },
      "source": [
        "## 3  Prediccion en los datos de Testing\n",
        "\n",
        "\n",
        "Se muestran los datos de testing, que son distintos a los de training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IscBRJaDdqS7"
      },
      "source": [
        "### 3.1 Prediccion en caliente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmG4baz1kYtS"
      },
      "outputs": [],
      "source": [
        "# aplico la red entrenada al dataset de testing\n",
        "\n",
        "(y_hat,y_raw, test_error_rate) = mp.predecir(df_new=df_test, campos=campos, clase='y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ9Fm1dzBsNu"
      },
      "source": [
        "#### Visualizacion del error en testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTkTRyXt6CuO"
      },
      "outputs": [],
      "source": [
        "print(\"error_rate (train, test): \",  train_error_rate, test_error_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5NT12b4B1Vf"
      },
      "source": [
        "#### Visualizacion de la prediccion en testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnEcIAV7slNW"
      },
      "outputs": [],
      "source": [
        "tb_salida_test = pl.DataFrame( {\"clase\":df_test[\"y\"], \"pred\":y_hat, \"y_raw\":y_raw })\n",
        "tb_salida_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8SvrJodiybt"
      },
      "source": [
        "#### Cálculo de errores por persona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyMxg7UshANc"
      },
      "outputs": [],
      "source": [
        "# Paso 1: crear una columna booleana que indique si acertó o no\n",
        "df = tb_salida_test.with_columns([\n",
        "    (pl.col(\"clase\") != pl.col(\"pred\")).alias(\"error\")\n",
        "])\n",
        "\n",
        "# Paso 2: agrupar por clase real y calcular tasa de error por clase\n",
        "errores_por_clase = (\n",
        "    df.group_by(\"clase\")\n",
        "    .agg([\n",
        "        pl.len().alias(\"total\"),\n",
        "        pl.col(\"error\").sum().alias(\"errores\")\n",
        "    ])\n",
        "    .with_columns([\n",
        "        (pl.col(\"errores\") / pl.col(\"total\")).alias(\"error_rate\")\n",
        "    ])\n",
        "    .sort(\"error_rate\", descending=True)\n",
        ")\n",
        "pl.Config.set_tbl_rows(20)\n",
        "print(errores_por_clase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A307crj1bE5G"
      },
      "outputs": [],
      "source": [
        "def analisis_fotos(fotos_reales, prediccion, fotos_error = True):\n",
        "  for i in range(len(prediccion)):\n",
        "    if ((fotos_error and prediccion['error'][i]) or (not(fotos_error) and not(prediccion['error'][i]))):\n",
        "      print(f\"La foto era de {prediccion['clase'][i]} y la red dijo que era de {prediccion['pred'][i]}\")\n",
        "      plt.figure(figsize=(1, 1))\n",
        "      plt.imshow(test_caras[i].reshape(50, 50), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpLfDrRhkHTl"
      },
      "outputs": [],
      "source": [
        "analisis_fotos(data_testing, df, True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}